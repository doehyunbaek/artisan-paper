\section{Introduction}
\label{s:introduction}

\todo{update intro}
Since the pilot of artifact evaluation at ESEC/FSE 2011, artifact evaluation has become standard practice in the software engineering and programming languages communities~\cite{DBLP:journals/cacm/KrishnamurthiV15}.
One of its main goals is reproducibility, defined as obtaining similar experimental outcomes with the same experimental setup by a different team~\cite{acm-artifact-badging-v1_1}.
Reproducibility is explicitly cited in Calls for Artifacts and is generally regarded by participants as one of the most important goals of artifact evaluation~\cite{DBLP:conf/sigsoft/Hermann0S20}.
In addition, studies report that artifact evaluation leads to more available, maintained, and documented artifacts~\cite{DBLP:conf/se/0001T0C0H024}.

Despite these benefits, artifact evaluation in its current form is notoriously labor-intensive.
Reports indicate that evaluating a single artifact can take eight to ten hours, often more than reviewing a paper~\cite{DBLP:journals/ieeesp/Hermann22}.
Evidence is mixed on whether papers that undergo artifact evaluation achieve greater visibility: some studies suggest higher visibility~\cite{DBLP:journals/ese/HeumullerNKO20}, while others find no correlation~\cite{DBLP:conf/se/0001T0C0H024}.
Given the significant manual effort, it is unclear whether the benefits to individual researchers outweigh the costs for both authors and artifact users.

\begin{table}[t]
\caption{Comparison of prior benchmarks and our benchmark. ML: Machine Learning. NLP: Natural Language Processing. CS: Computer Science. SS: Social Science. Med: Medicine. SE: Software Engineering.}
\label{t:benchmark_comparison}
\centering
\renewcommand{\arraystretch}{1.25}
\begin{tabularx}{\linewidth}{lXXXX}
\toprule
 & \textbf{SUPER}~\cite{DBLP:conf/emnlp/BoginYG0BCSK24} & \textbf{CORE-Bench}~\cite{DBLP:journals/tmlr/SiegelKNSN24} & \textbf{Repro-Bench}~\cite{DBLP:conf/acl/HuZLWPK25} & \textbf{\benchmark~(Ours)} \\
\midrule
Domain & ML, NLP & CS, SS, Med & SS & SE \\
Programming Languages & Python & Python, R & Stata, R, MATLAB, Julia, Python & Python, Java, Rust, Ocaml, Scala, C, Bash \\
\#Papers & 45 & 90\textsuperscript{1} & 112 & \papersetsize \\
\#Tasks & 45\textsuperscript{2} & 90\textsuperscript{3} & 112 & \tablesetsize \\
\#Inconsistencies Found & 0 & 0 & 0 & \inconsistenciessize \\
\#Fast-path Found & 0 & 0 & 0 & \fastpathsize \\
Time Limit & 30 minutes & 45 minutes & 2 hours & 8 hours \\
Judging & Accuracy of outputs & Accuracy of outputs & Accuracy of reproducibility scores & Submitted script reproduces the claim \\
\bottomrule
\end{tabularx}


\vspace{2mm}
\footnotesize \textsuperscript{1}\;37 for CS.
\footnotesize \textsuperscript{2}\;Expert set. SUPER additionally provides 152 Masked sets and 604 Auto sets.
\footnotesize \textsuperscript{3}\;CORE-Bench-Hard. CORE-Bench additionally provides 90 CORE-Bench-Easy and CORE-Bench-Medium.
\vspace{-2mm}
\end{table}

To mitigate these costs, recent work benchmarks the performance of LLM agents on reproducing research results.
Table~\ref{t:benchmark_comparison} summarizes these efforts.
SUPER~\cite{DBLP:conf/emnlp/BoginYG0BCSK24} evaluates the ability of LLM agents to reproduce results from ML and NLP papers.
CORE-Bench~\cite{DBLP:journals/tmlr/SiegelKNSN24} has a similar overall goal but covers broader domains (social science, medicine) and includes vision-language tasks.
Repro-Bench~\cite{DBLP:conf/acl/HuZLWPK25} focuses on reproducing social science research and adopts a more realistic problem setup.

However, we identify several limitations in existing benchmarks:
(1) They largely formulate reproduction as an output-prediction problem, which undermines credibility.
SUPER and CORE-Bench measure only the accuracy of the reproduced outcomes; the process by which agents obtain those results is not evaluated.
This diverges from the goals of artifact evaluation in software engineering and is more susceptible to data leakage.
Repro-Bench measures agreement with ground-truth reproducibility scores on a 1–4 scale.
While an ACM artifacts badge (akin to a reproducibility score) is a valuable \emph{signal} produced by artifact evaluation, it is not the main goal.
(2) Due to this problem formulation, their utility is limited.
Unlike human artifact evaluation, which increases confidence that a paper’s results are reproducible, running these benchmarks does not confer comparable confidence.
They can measure agent performance on the posed tasks but cannot augment or replace current artifact evaluation.
(3) Coverage is limited in both software engineering techniques (ML / NLP) and programming languages (Python, R, and Stata).

To address these limitations, we present \benchmark, an automated artifact evaluation benchmark for software engineering.
\benchmark~has the following characteristics:
(1) It measures an agent’s ability to \emph{generate code} that reproduces a specific claim; both the output and the submitted code are graded.
It also distinguishes \emph{fast paths} and \emph{short paths} that agents may take to reproduce results, a crucial distinction not highlighted in prior work.
(2) \benchmark~has two practical applications:
A script submitted by an agent can serve as a push-button reproduction script for the paper’s particular claim.
If the script’s outcome differs from the paper’s, it provides credible evidence that the paper, not the reproduction, contains a transcription or rounding error.
(3) \benchmark~covers diverse software engineering techniques (e.g., Empirical Study, Security, Fuzzing, Static Analysis, Dynamic Analysis) and programming languages (Python, Java, Rust, Ocaml, Scala, Bash, C).

Motivated by agents’ struggles on \benchmark, we also develop \emph{Artisan}, an agentic approach to automated artifact evaluation.
Artisan includes specialized tools for domain-specific challenges in artifact evaluation (Table~\ref{t:artisan_tools}).
\todo{Add technical novelties of Artisan.}

Our evaluation shows that while most LLM agents struggle on \benchmark, Artisan is the most effective, producing \particularscriptsize{} reproduction scripts and outperforming the baseline by \particularscriptsizeoutperform{}.
During the construction of \benchmark, we also uncovered \inconsistenciessize~inconsistencies between papers and artifacts and \fastpathsize~fast paths that artifacts provide, demonstrating the strength of our approach to benchmark construction.

In summary, this paper contributes the following:
\begin{itemize}
    \item \benchmark, a novel benchmark that evaluates the capabilities of AI agents to reproduce Software Engineering papers using script submission.
    \item Artisan, an LLM-based agent which incorporates several tools to aid the reproduction of software engineering research artifacts.
    \item Empirical evaluation which shows that (1) \benchmark~is challenging for the current state of the art LLM agents and (2) Artisan effectively addresses the common pitfalls faced by LLM agents.
    \item We make \benchmark~and Artisan publicly available: https://github.com/doehyunbaek/artisan
\end{itemize}
