\section{Evaluation}
\label{s:evaluation}

To evaluate our approach, we answer the following research questions:

\begin{itemize}[labelindent=\parindent,leftmargin=*]
\item \textbf{RQ1. Effectiveness}:
How effective is \approach{} at generating reproduction scripts for tables in software engineering research papers?
\item \textbf{RQ2. Efficiency}:
How efficient is \approach{} in terms of time and monetary cost?
\item \textbf{RQ3. Ablation Study}:
What is the impact of different tools in different configurations?
\end{itemize}

\subsection{Experimental Setup}

\subsubsection{Datasets}

We use \benchmark{} introduced in Section~\ref{s:benchmark} to evaluate our approach and the baselines.

\subsubsection{Implementation and Model}

We build \approach{} on top of mini-swe-agent~\cite{minisweagent,sweagent}.
Our tools are implemented in Python~3.12.10.
We use \texttt{GPT-5-mini-2025-08-07} as the model.

\subsubsection{Baselines}

As baselines, we select mini-swe-agent~\cite{minisweagent} and SWE-agent~\cite{sweagent}.
We choose mini-swe-agent for two reasons:
(1) \approach{} is built on top of mini-swe-agent; thus, comparing \approach{} with mini-swe-agent enables a thorough empirical study of \approach{}'s contributions.
(2) mini-swe-agent achieves competitive performance (70.6\% resolved on SWE-Bench Verified with Claude~4.5 Sonnet, compared to 78.8\% for the top score on the current leaderboard), while remaining simple (131~LOC in the core).
We also include SWE-agent as an established state-of-the-art approach for general software engineering tasks.
For both mini-swe-agent and SWE-agent, we use the same model (\texttt{GPT-5-mini-2025-08-07}).

\subsubsection{System Prompt Used}

To implement \approach{}, we use a system prompt developed for artifact evaluation\footnote{\url{https://github.com/doehyunbaek/artisan/blob/9b13d07f/src/artisan/mini.py\#L22-\#L71}}.
This is combined with the user prompt described in Section~\ref{s:user-prompt} to constitute the agent input.
For the baselines, we use the default system prompt of SWE-agent\footnote{\url{https://github.com/SWE-agent/SWE-agent/blob/8089c8ba/config/default.yaml\#L7}} and the default system prompt of mini-swe-agent\footnote{\url{https://github.com/SWE-agent/mini-swe-agent/blob/eeaf79b4/src/minisweagent/config/default.yaml\#L2-\#L17}}.

\subsubsection{Metrics}

To evaluate effectiveness, we measure the total number of \particularscrip~that LLM agents successfully generate.
To measure this, we use the Best-of-\(N\) metric with \(N=10\):
each agent may attempt to reproduce a table up to \(N\) times sequentially, terminating early if any attempt leads to a successful reproduction.
To evaluate efficiency, we report total wall-clock time and LLM token cost per task.

\begin{table}[t]
  \caption{Effectiveness of \approach{} on \benchmark{}.}
  \label{t:big_table}
  \input{tables/big-table}
\end{table}
