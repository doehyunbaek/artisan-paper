\section{Evaluation}
\label{s:evaluation}

To evaluate our approach, we answer the following research questions:

\begin{itemize}[labelindent=\parindent,leftmargin=*]
\item \textbf{RQ1. Effectiveness}:
How effective is \approach{} at generating reproduction scripts for tables in software engineering research papers?
\item \textbf{RQ2. Efficiency}:
How efficient is \approach{} in terms of time and monetary cost?
\item \textbf{RQ3. Ablation Study}:
What is the impact of different tools in different configurations?
\item \textbf{RQ4. Effectiveness and Accuracy of LLM-Basesd Tools}:
What is the effectiveness of LLM-based Format tool and accuracy of LLM-based Judge tool?
\end{itemize}

\subsection{Experimental Setup}

\subsubsection{Datasets}

For RQ1, RQ2, RQ3, We use \benchmark{} introduced in Section~\ref{s:benchmark} to evaluate our approach and the baselines.
For RQ4-1, we use 61 ground truth scripts to prepare inputs used to evaluate the effectiveness of Format tool.
For RQ4-2, we use 61 ground truth scripts (31~\fullrepro~scripts, 30~\downrepro~scripts), plus 25~\copyrepro~scripts that we collected during the development to evaluate the accuracy of the Judge tool.

\subsubsection{Baselines}

As baselines, we select mini-swe-agent~\cite{minisweagent} and SWE-agent~\cite{sweagent}.
We choose mini-swe-agent for two reasons:
(1) \approach{} is built on top of mini-swe-agent; thus, comparing \approach{} with mini-swe-agent enables a thorough empirical study of \approach{}'s contributions.
(2) mini-swe-agent achieves competitive performance (70.6\% resolved on SWE-Bench Verified with Claude~4.5 Sonnet, compared to 78.8\% for the top score on the current leaderboard), while remaining simple (131~LOC in the core).
We also include SWE-agent as an established state-of-the-art approach for general software engineering tasks.
For both baseslines and~\approach, we experiment with gpt-5.1-2025-11-13, gpt-5-mini-2025-08-07, and gpt-5-nano-2025-08-07.

\subsubsection{Metrics}

To evaluate effectiveness, we measure the sum of~\fullrepro~scripts and~\downrepro{}~scripts each approaches generate.
For the reproduction method we report, we report the result of the manual validation, not the one reported by the Judge tool (Section \ref{s:judge}).
To evaluate efficiency, we report total wall-clock time and LLM token cost per task.

\subsection{RQ1: Effectiveness}
\label{s:effectiveness}

\begin{table}[t]
  \caption{Effectiveness of \approach{} on \benchmark{}.}
  \label{t:big_table}
  \input{tables/big-table}
\end{table}

\begin{table}[t]
  \caption{Comparison of \approach{} with baselines.}
  \label{t:Baseline}
  \input{tables/baseline}
\end{table}

\subsection{RQ2: Efficiency}
\label{s:efficiency}

\subsection{RQ3: Ablation Study}
\label{s:ablation}

\begin{table}[t]
  \caption{Ablation study of \approach.}
  \label{t:ablation}
  \input{tables/ablation}
\end{table}
