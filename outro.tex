\section{Discussion}
\label{s:discussion}

\subsection{Inconsistencies Found Between Paper and Artifact}
\label{s:inconsistencies}

\begin{table}[t]
  \caption{Inconsistencies Found Between Paper and Artifact. \textsuperscript{\textdagger} denotes confirmatioin by the authors.}
  \label{t:inconsistencies}
  \input{tables/inconsistencies}
\end{table}

During the development of~\approach, we have encountered~\newbugsize~cases where there was some errors in papers or artifacts, which we encountered through numerical results being output by artifacts being inconsistent with the papers, a phenomena we term~\newbug.
To be clear, we do not classify cases where artifacts themselves warn about the possible inconsistencies as~\newbug; thus all~\newbugsize~we report are newly discovered errors that have not been disclosed publicly before.
Table~\ref{t:inconsistencies} summarizes the~\newbug~we found.
Overall, we found~\newbugsize~\newbug~from 13 papers.
This accounts for 22\% (13 / 60) of the papers we applied~\approach.
Note that all of the 60 papers that we apply~\approach~underwent an artifact evaluation process in the major SE conferences, and got awarded Artifacts Evaluated - Reusable badge.
Still, our approach could find inconsistencies in up to 29 entries in a table, and a value different up to $8.13 \times 10^{285960}$.

To give more insight into the~\newbug~we found, we perform a manual analysis to find out the reasons for the inconsistencies.
We come out with five different categories of reasons for~\newbug.
% Rounding Error
5 /~\newbugsize~belong to Rounding Error, which we attribute when the inconsistencies found could be explained away by the wrong use of the rounding algorithm.
For the rest of the Inconsistencies, we reported our analysis to the paper authors.
12 / 16 of the reported inconsistencies received confirmations by the authors.
% Data Difference
4 / 16 belong to Data Difference, which we attribute when the data used in the paper is different from the data present in the artifact.
For all four such cases, the data used in the paper is lost, rendering the faithful reproduction of the paper results unlikely.
This highlights the need for automated artifact evaluation technique like~\approach, which could have prevented such cases had it been applied earlier.
% Artifact Error
3 / 16 belong to Artifact Error, which we attribute when we could find apparent errors in the artifact data or code.
Two out of three artifact errors are already fixed and the authors shared an intention to fix the remaining one.
\footnote{For Artifact Error of lasapp\cite{lasapp}, both the number of the paper and the artifact used for the paper results were buggy. We applied~\approach~to the fixed version of the artifact as the buggy version was not accessible to the public, which led to the disclosure of the buggy version.}
% Transcription Error
1 / 16 belong to Transcription Error, which we attribute when the inconsistencies likely stemed from manual process of transcribing artifact values to the paper.
% Paper Error
8 / 16 belong to Paper Error, which is a general catch-all category of all inconsistencies attributable to error in the paper.
\todo{more analysis on 8 paper errrors.}

\subsection{Fast Paths Taken by Agents}
\label{s:fastpath}

\begin{table}[t]
    \centering
    \caption{Frequency of Fast Paths Taken by Agents}
    \label{t:fastpath}
    \begin{tabular}{cccccc}
        \toprule
         \textbf{Checked-in Results} & \textbf{Checked-in Papers}  & \textbf{Notebook Outputs} & \textbf{Paper Texts} & \textbf{Others} & \textbf{Total} \\
        \midrule
        12 & 7 & 7 & 3 & 1 & \fastpathsize \\
        \bottomrule
    \end{tabular}
\end{table}

% action_table_2.log: Notebook Outputs
% action_table_3.log: Notebook Outputs
% action_table_5.log: Notebook Outputs
% action_table_6.log: Notebook Outputs
% baro_table_2.log: Checked-in Papers
% bloat_table_3.log: Other (Paper Texts)
% buggyui_table_2.log: Checked-in Papers
% dcm_table_3.log: Checked-in Papers
% fuzzslice_table_1.log: Checked-in Papers
% gradual_table_3.log: Checked-in Results
% hypertesting_table_1.log: Checked-in Results
% lasapp_table_2.log: Other (Paper Texts)
% llm_table_1.log: Notebook Outputs
% llm_table_2.log: Checked-in Papers
% llm_table_3.log: Notebook Outputs
% mwt_table_2.log: Other (download from arxiv)
% newton_table_4.log: Notebook Outputs
% provenfix_table_2.log: Checked-in Results
% provenfix_table_3.log: Checked-in Results
% rca_table_3.log: Checked-in Papers
% reclues_table_4.log: Checked-in Papers
% rust_table_1.log: Checked-in Results
% rust_table_5.log: Other (Paper Texts)
% sugar_table_2.log: Checked-in Results
% trace2inv_table_7.log: Checked-in Results
% triad_table_3.log: Checked-in Results
% unimocg_table_1.log: Checked-in Results
% unimocg_table_2.log: Checked-in Results
% unimocg_table_3.log: Checked-in Results
% unimocg_table_4.log: Checked-in Results

During the development of~\approach, we have encountered~\fastpathsize~cases where agents generate~\goodscript~that merely copies checked-in results as a method of ``reproduction'', a phenomena we term~\fastpath.
Although similar misalignment issues are reported in other domains~\todo{cite}, to our knowledge, we are the first to report a misalignment issue of this kind in he domain or artifact evaluation.

We perform a manual analysis of trajectories of agents that produced these~\fastpath, and report the findings in Table~\ref{t:fastpath}.
For 12 / 30 cases, artifacts contain some checked-in research results (Checked-in Results).
Agents discover them while exploring the artifact and directly copy these results.
For 7 / 30 cases, artifacts contain papers in pdf forms (Checked-in Papers).
Agents convert the pdfs into a text-readable form and directly copy the expected tables.
For 7 / 30 cases, artifacts contain Jupyter Notebooks which contain the output of the previous run, which agents directly copy (Notebook Outputs).
For 3 / 30 cases, although we give a obfuscated table when we provide the paper, agents still try to read the texts surrounding the obfuscated table and infer the expected results (Paper Texts).
Lastly, on one case, after other attempts fail, the agent directly downloads the paper by searching arxiv.org with the paper title and obtains the preprint.
These various ways of obtaining research results while actually not doing the reproduction shows a need for verification of not only the output but the method of reproduction, as done by~\approach.

\subsection{Threats To Validity}
\label{s:threats}

Our effectiveness results are influenced by the step limit (30 steps) and the cost limit (\$1) we used.
Effectiveness result could potentially improve with larger step limits and cost limits.
However, we judge this is a reasonable amount that researchers are willing to spend for the automated reproduction of the single table.
In addition, results of~\approach~are limited to the paper selected due to the process described in Section~\ref{s:paper_select}.
Thus,~\approach~is only evaluated on Software Engineering papers that are packaged using Docker, that are packaged using docker, that do not use non-public API or GPU.
To be clear,~\approach~can be applied to artifacts that do not use Docker for packaging and do use non-public API or GPU.
We motivate this limited evaluation as a measure to reduce the manual cost of reproducing each research results and automated cost of running~\benchmark~both in terms of monetary cost and execution time.

\subsection{Limitations}
\label{s:limitations}

\approach~can not reproduce non-numerical results, as it lacks a capacity to judge a reproduction of the non-numerical research results is correct or not.
Thus,~\approach can not reproduce research results such as figures.
\approach~can not reproduce non-deterministic results, as the Judge tool only accepts one deterministic results as a valid reproduction.
Thus,~\approach can not reproduce research results such as running time.

\section{Related work}
\label{sec:relatetd}

\paragraph{Artifact Evaluation}

Artifact evaluation has first been introduced to the Software Engineering community at ESEC/FSE 2011~\cite{DBLP:journals/sigsoft/Krishnamurthi13} and subsequently adapted to programming languages~\cite{DBLP:journals/cacm/KrishnamurthiV15}, security~\cite{DBLP:conf/ccs/OlszewskiLSWKPU23}, and database communities~\cite{DBLP:journals/sigmod/AthanassoulisTA22}.
In addition to the artifact evaluation being practiced in the reserach community, recent work has done research on the process of artifact evaluation itself.
Hermann et al.~\cite{DBLP:conf/sigsoft/Hermann0S20} investigates the expectations of the participants of artifact evaluation committee and find that reproducibility and reusability is the primary perceived goal of the artifact evaluation.
Winter et al.~\cite{DBLP:conf/sigsoft/0001T0C0H022} studies what impact artifact evaluation process has on artifacts and find that although artifacts that underwent artifact evaluation do not have better visibility, it is more available and maintained.
Olszewski et al.~\cite{DBLP:conf/ccs/OlszewskiLSWKPU23} measures reproducibility of the machine learning security papers.
Anecdotally, they find that introduction of artifact evaluation at USENIX in 2020 led to more artifacts being available and functional.
Angermeir et al.~\cite{DBLP:journals/corr/abs-2510-25506} measures reproducibility of Software Engineering papers utilizing LLM and find that after one year of artifact evaluation, half of the 16 papers that has received the Artifacts Evaluated - Reusable badge are either incomplete or non-functional.
However, none of these work aim to automate the core process of artifact evaluation itself.
Our work tackles this problem and paves the way for future research in the space of automated artifact evaluation.

\paragraph{Automated Research Reproduction}

Due to the advancement of LLM agents, recent work has explored utilizing LLM agents for the purpose of reproducibility, defined as the ability of a different team to obtain similar experimental outcomes using the same experimental setup~\cite{acm-artifact-badging-v1_1}.
SUPER~\cite{super} introduces new benchmark that evaluates the capability of agents in setting up and executing tasks from research repositories.
% Paper Selection
% - 45 papers
%   - Filtering Criterion
%   - Starts with Papers with Code repo
%   - “Text” modalities
%   - >= 2021 and beyond
% Task Generation
%   - Expert set: 45 manually generated problems with annotated export solutions
%     - Requires no GPU compute, <= 10 mins
%   - Masked set:152 sub-problems generated by masking
%   - Auto set: 604 LLM generated problems.
%     - Simple: mainly install dependencies and start experiments, no training / inference
% Judging
%   - For Expert set, masked set, accuracy of outcome with respect to golden solution.
%   - For Auto set, just check if the script run without exception
CORE-Bench~\cite{corebench} also introduces benchmark that evaluates agents in the task of reproducing research results, widening the scientific disciplines covered (social science, medicine).
% Paper Selection
%   - 90 Papers
%   - Filtering Criterion (10)
%     - Run under 45 minutes
%     - Requires simple bash command
%     - Results are adequately labeled
%     - Python or R
% Task Generation
%   - 270 tasks
%   - Manual construction from reproducible run
% Michael’s comments
%   - Discuss SUPER and Core-Bench’s variation in related work
%   - Motivate on the reproducibility see Core-bench table 1
%   - Benchmarking harness - parllelization. Take a look at the benchmarking infra description.
REPRO-Bench~\cite{reprobench} is another benchmark that targets reproducing research results of social science papers.
It differs from both SUPER and CORE-BENCH by being rating-based, i.e. the agent outputs reproducibility score from 1 to 4, roughly corresponding to Artifacts Evaluated badge.
% Reproduction benchmark for social science research
% Eval set: economics / political science journal publications from previous work(92 / 112)
% Input: paper pdf, artifact repo, a list of major findings.
%   - A list of major findings: a list of figures / tables / text claims which has evidence of reproduction by the previous reproduction reports.
% Output: Should guess a score of 1 to 4. 1: Irreproducible 4: fully reproducible
%   - Graded in terms of accuracy (guess the score right. Random guess: 25%), applicability (follows submission format). Cost.
% Results: baselines( AutoGPT, Core-Agent, SWE-Agent): low accuracy from 1.8% to 20.5%, worse than random guess.
%   - Author’s agent (with workflow templates, prompt tuning for following submission format and avoiding common errors) yield 36.6%
%   - Cost is high: ~$2 dollars per instance (with gpt-4o) for AutoGPT and CORE-Agent
% Takeaways:
%   - End-to-End assessment with non-hand crafted inputs. We try to give similar realistic inputs to the agents.
%   - Grading agents based on accuracy seems inappropriate. Not so meaningful and also prone to feature leakage → We require reproduction script per task as output.
%   - They support figure reproduction → Maybe we should also.
%   - Path confusion accounts for 56% of the issues encountered by agents; agrees with our anecdotes.
% - Time constraint: limit reproduction to papers / artifacts explicitly stating it should take <2 hours.
Heye et al.~\cite{heye} develops a LLM based system that aims to support the artifact evaluation process through outputting reproducibility score, automatically setting up environment, and identifying common pitfalls.
Our approach and benchmark differs with these previous work in the following ways:
(1) We aim to reproduce the tables from research papers as-is. 41 / 60 tables we reproduce are exact tables from papers, not hand-crafted tasks.
(2) We formulate the task as a code generation problem, leading to detection of~\newbug~and~\fastpath.
(3) We provide three domain-specific tools specialized for the task of artifact evaluation.
(4) We cover diverse software engineering techniques and programming languages previously not covered.
Thus, despite the shared task of automating reproduction of research results, our approach and benchmark advance the current state-of-the-art in different directions.

\paragraph{Automated Research Replication}

There is a related but different problem of research replication, which is defined as the ability of a different team to obtain similar experimental outcomes using the different experimental setup~\cite{acm-artifact-badging-v1_1}.
Liang et al.~\cite{DBLP:journals/pacmse/LiangBBDFF024} examines an ability of LLM (GPT-4) to perform replications of empirical software engineering research and finds that although assumptions and code LLM generates are mostly correct, there are few ways that are lacking.
% Paper Selection
%   - 7 papers
%   - Filtering Criteria
%     - Quantitative empirical analysis (analysis done through code rather than manual)
%     - >= 2010
%     - 1-page methodology section (not too long)
%     - Three sets of papers
%       - SE venues (2): From 20 top-cited
%       - MSR papers (3): From 20 top-cited
%       - Distinguished Paper Awardess after 2021 (2)
%     - Authors acknowledge bias of paper selection affecting external validity
PaperBench~\cite{DBLP:conf/icml/StaraceJSACMDMK25} introduces a new benchmark that evaluates LLM agents' ability to replicate 20 ICML 2024 papers and finds that the best-performing agent achieves an average replication score of 21.0\% but still falls behind the human baseline.
% Paper Selection
%   - 20 ICML 2024 papers
%     - Admitted as limitation. Argue that there are thousands of tasks
%   - Filtering Criteria
%   -  >75% authors likely to collaborate
%   -  Substantial empirical experiment
%   -  No distributed training
%   -  No unreliable dependencies (e.g. closed-source model, unreliable API)
%   -  No Human data collection / annotation
%   -  Sufficient details to be replicated from scratch
%   -  No software framework / libraries
%   - Random sampling, manual filtering
%   - Asked 42 authors for help, 20 agreed to manual task production
% Task generation (“Rubric” according to the authors)
%   - 8316 individually gradable outcomes
%   - Significant manual effort (tens of hours per paper)
%   - 12 hour runtime cap per root task
% Judging
%   - LLM judges are quite costly ($66 USD for o3-mini and $830 for o1, but 1200 = $100 * 12 hours for humans according to author’s claim)
PaperCoder~\cite{DBLP:journals/corr/abs-2504-17192} is a multi-agent LLM framework that tackles the automated research replication task through three stages of planning, analysis, and generation.
Xiang et al.~\cite{DBLP:journals/corr/abs-2504-00255} introduces SciReplicate-Bench, a replication benchmark targetig 36 NLP papers and Sci-Reproducer, a dual-agent framework consisting of Paper Agent and Code Agent.
% Benchmark
%   - 36 papers
%     - 12 hours manual annotation
%   - 100 tasks
%   - Reasoning graph accuracy
% Approach
%   - Paper Agent
%     - Paper reading
%   - Code Agent
%     - Find, grep, web search, compiler.
Our work is fundamentally different in terms of problem that we tackle (reproduction vs replication) and scope of trying to augment the real-world artifact evaluation process itself.

\paragraph{General Software Engineering LLM Agents}

Currently there is a general trend of utilizing LLM Agents for various Software Engineering tasks.
RepairAgent~\cite{DBLP:conf/icse/BouzeniaDP25} proposes first LLM Agent that solves the task of automated program repair.
ExecutionAgent~\cite{DBLP:journals/pacmse/BouzeniaP25} presents an LLM agent that automatically sets up arbitrary project and execute test suites.
SWE-Agent~\cite{sweagent} introduces agent-computer interface that aims to enhance LLM aggents' abilities of computer usage in the task of SWE-Bench~\cite{jimenez2024swebench}
MLE-Bench~\cite{DBLP:conf/iclr/ChanCJASMSLMPMW25} introcues a benchmark that measures the LLM agents' performance in machine learning engineering.
Our work primarily differs from these work in terms of domain we target (automated artifact evaluation).

\section{Conclusion}
\label{sec:conclusion}

We present~\approach, the first agent that automates the process of artifact evaluation with LLM agents.
Our main technical contribution is problem definition of reproducing numerical research results as a code generation task: we ask LLM agents to generate~\goodscript~, which runs independently to reproduce the research results.
In addition, we develop tools to solve the domain-specific challenges of artifact evaluation.
Due to our problem definition,~\approach~is able to discover~\newbug, which are inconsistencies between results of paper and artifacts due to errors on either side and~\fastpath, which are trivial attempt at reproduction that just copies checked in results.
To evaluate~\approach, we develop~\benchmark, a new benchmark that evaluates LLM agents' ability to reproduce tables from Software Engineering research papers.
Our evaluation shows that~\approach~is effective and efficient, with all tools contributing to this result.
We envision~\approach~to pave way for further research in the task of automated artifact evaluation, empowering researchers to automatically reproduce previous research, furthering the advancement of science.

\section*{Data Availability}
We share the anonymized version of~\approach~and~\benchmark{}: \cite{artifact}.
