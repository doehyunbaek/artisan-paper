\section{Discussion}
\label{s:discussion}

\subsection{Inconsistencies Found Between Paper and Artifact}
\label{s:inconsistencies}

\begin{table}[t]
  \caption{Inconsistencies Found Between Paper and Artifact. \textsuperscript{\textdagger} denotes confirmatioin by the authors.}
  \label{t:inconsistencies}
  \input{tables/inconsistencies}
\end{table}

During the development of~\approach, we have encountered~\newbugsize~cases where there was some errors in papers or artifacts, which we encountered through numerical results being output by artifacts being inconsistent with the papers, a phenomena we term~\newbug.
To be clear, we do not classify cases where artifacts themselves warn about the possible inconsistencies as~\newbug; thus all~\newbugsize~we report are newly discovered errors that have not been disclosed publicly before.
Table~\ref{t:inconsistencies} summarizes the~\newbug~we found.
Overall, we found~\newbugsize~\newbug~from 13 papers.
This accounts for 22\% (13 / 60) of the papers we applied~\approach.
Note that all of the 60 papers that we apply~\approach~underwent an artifact evaluation process in the major SE conferences, and got awarded Artifacts Evaluated - Reusable badge.
Still, our approach could find inconsistencies in up to 29 entries in a table, and a value different up to $8.13 \times 10^{285960}$.

To give more insight into the~\newbug~we found, we perform a manual analysis to find out the reasons for the inconsistencies.
We come out with five different categories of reasons for~\newbug.
% Rounding Error
5 /~\newbugsize~belong to Rounding Error, which we attribute when the inconsistencies found could be explained away by the wrong use of the rounding algorithm.
For the rest of the Inconsistencies, we reported our analysis to the paper authors.
12 / 16 of the reported inconsistencies received confirmations by the authors.
% Data Difference
4 / 16 belong to Data Difference, which we attribute when the data used in the paper is different from the data present in the artifact.
For all four such cases, the data used in the paper is lost, rendering the faithful reproduction of the paper results unlikely.
This highlights the need for automated artifact evaluation technique like~\approach, which could have prevented such cases had it been applied earlier.
% Artifact Error
3 / 16 belong to Artifact Error, which we attribute when we could find apparent errors in the artifact data or code.
Two out of three artifact errors are already fixed and the authors shared an intention to fix the remaining one.
\footnote{For Artifact Error of lasapp\cite{lasapp}, both the number of the paper and the artifact used for the paper results were buggy. We applied~\approach~to the fixed version of the artifact as the buggy version was not accessible to the public, which led to the disclosure of the buggy version.}
% Transcription Error
1 / 16 belong to Transcription Error, which we attribute when the inconsistencies likely stemed from manual process of transcribing artifact values to the paper.
% Paper Error
8 / 16 belong to Paper Error, which is a general catch-all category of all inconsistencies attributable to error in the paper.
\todo{more analysis on 8 paper errrors.}

\subsection{Fast Paths Taken by Agents}
\label{s:fastpath}

\begin{table}[t]
    \centering
    \caption{Frequency of Fast Paths Taken by Agents}
    \label{t:fastpath}
    \begin{tabular}{cccccc}
        \toprule
         \textbf{Checked-in Results} & \textbf{Checked-in Papers}  & \textbf{Notebook Outputs} & \textbf{Paper Texts} & \textbf{Others} & \textbf{Total} \\
        \midrule
        12 & 7 & 7 & 3 & 1 & \fastpathsize \\
        \bottomrule
    \end{tabular}
\end{table}

% action_table_2.log: Notebook Outputs
% action_table_3.log: Notebook Outputs
% action_table_5.log: Notebook Outputs
% action_table_6.log: Notebook Outputs
% baro_table_2.log: Checked-in Papers
% bloat_table_3.log: Other (Paper Texts)
% buggyui_table_2.log: Checked-in Papers
% dcm_table_3.log: Checked-in Papers
% fuzzslice_table_1.log: Checked-in Papers
% gradual_table_3.log: Checked-in Results
% hypertesting_table_1.log: Checked-in Results
% lasapp_table_2.log: Other (Paper Texts)
% llm_table_1.log: Notebook Outputs
% llm_table_2.log: Checked-in Papers
% llm_table_3.log: Notebook Outputs
% mwt_table_2.log: Other (download from arxiv)
% newton_table_4.log: Notebook Outputs
% provenfix_table_2.log: Checked-in Results
% provenfix_table_3.log: Checked-in Results
% rca_table_3.log: Checked-in Papers
% reclues_table_4.log: Checked-in Papers
% rust_table_1.log: Checked-in Results
% rust_table_5.log: Other (Paper Texts)
% sugar_table_2.log: Checked-in Results
% trace2inv_table_7.log: Checked-in Results
% triad_table_3.log: Checked-in Results
% unimocg_table_1.log: Checked-in Results
% unimocg_table_2.log: Checked-in Results
% unimocg_table_3.log: Checked-in Results
% unimocg_table_4.log: Checked-in Results

During the development of~\approach, we have encountered~\fastpathsize~cases where agents generate~\goodscript~that merely copies checked-in results as a method of ``reproduction'', a phenomena we term~\fastpath.
Although similar misalignment issues are reported in other domains~\todo{cite}, to our knowledge, we are the first to report a misalignment issue of this kind in he domain or artifact evaluation.

We perform a manual analysis of trajectories of agents that produced these~\fastpath, and report the findings in Table~\ref{t:fastpath}.
For 12 / 30 cases, artifacts contain some checked-in research results (Checked-in Results).
Agents discover them while exploring the artifact and directly copy these results.
For 7 / 30 cases, artifacts contain papers in pdf forms (Checked-in Papers).
Agents convert the pdfs into a text-readable form and directly copy the expected tables.
For 7 / 30 cases, artifacts contain Jupyter Notebooks which contain the output of the previous run, which agents directly copy (Notebook Outputs).
For 3 / 30 cases, although we give a obfuscated table when we provide the paper, agents still try to read the texts surrounding the obfuscated table and infer the expected results (Paper Texts).
Lastly, on one case, after other attempts fail, the agent directly downloads the paper by searching arxiv.org with the paper title and obtains the preprint.
These various ways of obtaining research results while actually not doing the reproduction shows a need for verification of not only the output but the method of reproduction, as done by~\approach.

\subsection{Threats To Validity}
\label{s:threats}

\subsection{Limitations}
\label{s:limitations}

\section{Related work}
\label{sec:relatetd}

\paragraph{Artifact Evaluation}

\paragraph{Research Reproduction Automation}

\paragraph{Research Replication Automation}

\paragraph{General Software Engineering LLM Agents}

\section{Conclusion}
\label{sec:conclusion}

\section*{Data Availability}
We share the anonymized version of~\approach~and~\benchmark{}: \todo.
