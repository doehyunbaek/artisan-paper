% \section{Background}
% \label{s:background}

% \subsection{Artifact Evaluation}

% \subsection{LLM Agents for Software Engineering}

\section{Approach}
\label{s:approach}

We first define the problem we address (Section~\ref{s:problem_def}), introduce challenges we solve (Section~\ref{s:challenges}), and give a brief overview of the approach (Section~\ref{s:overview}).
Then, we provide a detailed explanation of the three tools that we provide to the agent (Sections \ref{s:get}, \ref{s:format}, \ref{s:judge}).
Finally, we explain the implementation details (Section~\ref{s:implementation}).

\subsection{Problem Definition}
\label{s:problem_def}

The problem we address is as follows: Given a research paper, a table from the research paper, an artifact accompnaying the research paper, we want to automatically generate a script that reproduces the table with the given artifact.
The generated script would encapsulate all the logic to reproduce the given table from scratch, from downloading the artifact, exercising the artifact, and formatting the execution logs into a expected table.

We address this problem in a way that has two important properties.
First, the approach should reproduce the exact table from the research paper as-is.
Previous works on automated research reproduction~\cite{super,corebench}~relies on research results hand-crafted by the researchers themselves for the purpose of constructing challenging but solvable tasks for the LLM agents.
However, to achieve the goal of automating artifact evaluation, and to measure the progress on this goal, we judge having the most realistic setup is required.
As a result,~\approach~is evalauated on the original tables from the research paper in the majority of cases (41 / 60) in our evaluation (Section~\ref{s:task_select}), notwithstanding some partial tables due to the limitations (Section~\ref{s:limitations}).

Second, the approach should produce script that runs independently without the involvement of the agent.
Previous work on automated research reproduction~\cite{reprobench,heye,super,corebench} utilizes LLM agents to reproduce research results.
But even if these agents succeed, the outcome is either in the form of rating~\cite{reprobench,heye} or output~~\cite{super,corebench}.
The users of these agents should either blindly trust the outcome or go through the process of analyzing the often lengthy agent trajectories.
We propose that tasking LLM agents with generating script that reproduces the table solves this problem.
The users of~\approach~do not have to trust the LLM agents or analyze the trajectories; they can just run and analyze the script these LLM agents have generated.
The script-based reproduction also aids in discovering both~\newbug~(Section~\ref{s:inconsistencies}) and~\fastpath~(Section~\ref{s:fastpath}), as manual effort to verify if the reproduction is correct is reduced.

\subsection{Challenges}
\label{s:challenges}

While the proposed approach has the aformentiend advantages, a few challenges accompany the proposed approach.
We have encountered following challenges during development, which motivates our development of domain-specific tools for artifact evaluation:
\subsubsection{Agents struggle with formatting execution logs into expected outputs}
\label{s:format_challenge}

Even after an agent has successfully reproduced research results by running a series of commands, the outputs of the commands are usually not exactly the same as what is presented in the paper.
Some artifacts also provide scripts that process these execution outputs into the format used in the paper, but many artifacts do not.
Of course, an LLM agent can try to write the missing processing script itself, but we found that it struggles to write such scripts in many cases.
Often, an LLM agent successfully ran commands necessary to reproduce the research results but could not get the correct score from the judge because the formatting of the execution outputs was inadequate.

\subsubsection{Agents struggle with choosing appropriate means of reproduction}
\label{s:judge_challenge}

In previous work developing output-based agents~\cite{super, corebench}, we found two characteristics that were inadequate for our problem formulation:
(1) The agents do not get feedback on their submission while they are working on the problem.
This becomes problematic in our case, as generating code that reproduces the result is a more complex problem and has more room for error than outputting just the result.
(2) How the agents obtain the output is not judged.
We observe many~\fastpath~that agents take while attempting to reproduce the result (Section~\ref{s:fastpath}), and proper consideration of this phenomenon is important for the accurate assessment of the LLM agents' capabilities.

\subsection{Overview}
\label{s:overview}

\begin{figure}[t]
  \centering
  \begin{minted}[fontsize=\scriptsize]{markdown}
## Goal
Please reproduce the results reported in Table 6 from the SE paper.
<obfuscated_result>
**Table 6: Sensitivity of deactivating scheduled workflows to the parameter k.**

| k                     |    1 |    2 |    5 |   10 |   15 |   20 |
| --------------------- | ---: | ---: | ---: | ---: | ---: | ---: |
| Impact on VM time (%) | -?.? | -?.? | -?.? | -?.? | -?.? | -?.? |
</obfuscated_result>
The same result is available at `/workspace/obfuscated.md`.
The paper that the result is from is available at `/workspace/paper.md`.

## Workflow overview
1. **Download the artifact** from https://zenodo.org/records/10529665.
2. **Read the README** to understand reproduction steps.
3. **Locate relevant commands** for Table 6.
4. **Execute the required commands** step-by-step.
5. **Author the reproduction script** `repro_action_table_6.sh` using the template below.
  \end{minted}
  \caption{Example user prompt provided to the agent}
  \Description{Example user prompt provided to the agent}
  \label{f:user_prompt}
\end{figure}

\begin{figure}[t]
  \centering
  \begin{minted}[fontsize=\scriptsize]{bash}
#!/bin/bash
<Commands to reproduce the results> # Filled out by LLM Agent later
artisan format obfuscated.md repro.log # Section 2.3: Format tool
artisan judge repro_action_table_6.sh # Section 2.4: Judge tool
  \end{minted}
  \caption{Example script template provided to the agent.}
  \Description{Example script template provided to the agent.}
  \label{f:script_template}
\end{figure}

An LLM agent is an LLM-based system that uses tools to autonomously interact with the environment to achieve a given goal, provided in the form of a user prompt~\cite{DBLP:conf/icse/BouzeniaDP25}.

% Figure~\ref{f:user_prompt} gives an example of a user prompt given to our LLM agent to task it to generate~\goodscript~that reproduces Table 6 of Bouzenia and Pradel~\cite{DBLP:conf/icse/BouzeniaP24}.

As a goal, the LLM agent is given a numerical result to reproduce, albeit with results obfuscated with question marks.
While developing our approach, we found that giving the results unobfuscated led to the undesirable behavior of directly copying the given results.
By giving results in an obfuscated form, we instruct LLM agents on what to reproduce but not on what the expected values are.
In addition, the content of the paper is provided to the agent, with the results to reproduce obfuscated as well.

As a workflow, the LLM agent is provided with a series of steps that should be taken to reproduce a numerical result from the research paper.
We only provide high-level objectives that should be taken at each step, and the LLM agent autonomously executes actions to achieve these objectives.
The five-step workflow provided to the agent is modeled after how researchers would tackle the problem of reproducing research artifacts themselves.
The last step of the workflow, authoring the reproduction script, is the key distinction of our approach compared to existing work.
As we try to automate the artifact evaluation by tasking LLM agents to generate~\goodscript{}, we provide a script template that agents should follow to guide the generation.

Figure~\ref{f:script_template} shows the script template we provide.
First, agents should fetch the artifact from different artifact repositories.
We provide the Get tool (Section~\ref{s:get}) to offer a unified interface to the artifact repositories.
Then, the agent autonomously exercises the fetched artifact to reproduce the research results.
As the commands needed to reproduce the results vary from artifact to artifact, we ask the agent to review the commands it has executed to fill in the necessary commands.
Next, the agent has to analyze and format the execution logs into the expected format.
To support this, we provide the Format tool (Section~\ref{s:format}), which is an LLM-based formatter that takes an expected results and execution log as input and outputs a formatted results.
Lastly, to guide the agent in reproducing the research result, we allow the agent to get feedback on the \goodscript~they have generated with the Judge tool (Section~\ref{s:judge}).
By calling the Judge tool, the agent can get feedback on whether (1) the output of the generated code matches the actual results, and (2) the way~\goodscript~generates the output is appropriate.
The agent can incorporate the feedback if either of the two conditions is not met, guiding the reproduction process.

\todo{overview figure}

\todo{algorithm}

% \subsection{\emph{Get} tool: Fetching Research Artifacts}
% \label{s:get}

% There currently exist multiple different repository providers that researchers use to archive their artifacts, including Zenodo, Figshare, and GitHub.
% Without a specialized tool, an LLM agent can still retrieve the artifact by utilizing CLI tools like \texttt{curl}.
% However, we found that agents stumble when interacting with repository APIs initially.
% In addition, complex probing of repository APIs makes the code generation task more complicated, leading to more errors.

% To mitigate this, we implement the Get tool, which the agent can invoke with just the URL to fetch the research artifact.
% It handles the repository-specific logic silently, downloads the research artifact, extracts the research artifact, and locates the README file.
% It also supports caching of artifacts that have already been fetched.
% While simple, we found that this is an effective strategy that helps the agent in initial fetching and also in generating~\goodscript.

\subsection{\emph{Format} tool: Formatting Execution Outputs into Results}
\label{s:format}

% \begin{figure}[t]
%   \centering
%   \begin{minted}{markdown}
% You format results into GitHub-flavored Markdown tables.

% CRITICAL TEMPLATE RULE:
% Return a table that is IDENTICAL to the provided expected table except that
% every placeholder question mark (?) is replaced by exactly one decimal digit
% (0-9). One ? => one digit. A pattern like "??.??" means two digits, a dot,
% two digits. A pattern like "?,???,???" means digits in those exact slots with
% commas preserved. Do NOT add, remove, or move pipes |, spaces, commas, dots,
% percent signs %, header separators, or rows. Do NOT alter any cell that
% contains no ? characters. Preserve capitalization and ordering exactly.

% DATA FILLING:
% Derive each replaced digit sequence from ONLY the reproduction text.
%   \end{minted}
%   \caption{Part of the system prompt used to implement the Format tool}
%   \Description{Part of the system prompt used to implement the Format tool}
%   \label{f:format}
% \end{figure}

\begin{figure}[t]
  \centering
  % First Subfigure
  \begin{subfigure}[t]{0.48\textwidth}
    \begin{minted}[fontsize=\scriptsize, breaklines=true]{markdown}
**Table 3: Evaluation Results**

| Project Name | Summary   | Annotations | Warnings |
| ---------------------    |             | -------- |
| MarginSwap   | (omitted) |           ? |        ? |
| PoolTogehter | (omitted) |           ? |        ? |
| Tracer       | (omitted) |           ? |        ? |
| Yield Micro  | (omitted) |           ? |        ? |

    \end{minted}
    \caption{Part of the obfuscated results provided as input.}
    \Description{Part of the obfuscated results provided as input.}
    \label{f:format_input_result}
  \end{subfigure}
  \hfill % Adds space between the figures
  % Second Subfigure
  \begin{subfigure}[t]{0.48\textwidth}
    \begin{minted}[fontsize=\scriptsize, breaklines=true]{bash}
Switched global version to 0.8.3
'solc --version' running
Reference: ScType
Error with TMP_57 in function updateHourlyBondAmount
Error with: TMP_57 in function updateHourlyBondAmount
Annotation count: 6
Function count: 20
Executing Group 1
[*] Tested 1 warning for MarginSwap
    \end{minted}
    \caption{Part of the execution output provided as input.}
    \Description{Part of the execution output provided as input.}
    \label{f:format_input_log}
  \end{subfigure}
  \caption{Example input given to the Format tool for Table 3 of ScType~\cite{sctype}.}
  \label{f:format_input}
\end{figure}

To address the challenge introduced in Section ~\ref{s:format_challenge},, we implement the Format tool, which is an LLM-based formatter that takes an obfuscated results and execution log as input and outputs a formatted results.
Figure~\ref{f:format_input} shows an example input that is passed to the Format tool.
While the research results are structured in a tabular format (Figure~\ref{f:format_input_result}), the output of the execution is just a log file (Figure~\ref{f:format_input_log}), lacking structure.
In addition, the log file is 1,103 lines in total.
The Format tool passes the obfuscated results and execution output as inputs with few-shot examples to the LLM, and outputs the results with the expected values filled in.

\subsection{\emph{Judge} tool: Evaluating Reproduction Attempts}
\label{s:judge}

To address the challenge introduced in Section ~\ref{s:judge_challenge},, we propose a two-tier judging system which addresses both problems: execution-based judging of the code output (Section~\ref{s:judge_execution})~and LLM-based judging of the reproduction method (Section~\ref{s:judge_llm}).

\subsubsection{Execution-Based Judging of Reproduction Scripts}
\label{s:judge_execution}

In our approach, LLM agents generate and submit a shell script that runs independently of any agent-related machinery.
Given the submission, our execution-based judging mechanism runs the submitted script in a fresh container environment.
The judging mechanism classifies three cases where the results produced by the submitted script do not match the expected results:
\begin{itemize}
  \item \textbf{\staticerror}: The submitted script fails to run because of a syntax error or script is missing from the expected path.
  \item \textbf{\runtimeerror}: The submitted script encounters a runtime error while running.
  The judging mechanism returns which command errored with what message as a feedback.
  \item \textbf{\mismatcherror}: The submitted script runs to completion without any error but the result differs from the expected values.
\end{itemize}

\begin{figure}[t]
  \centering
  \begin{subfigure}[t]{0.30\textwidth}
    \begin{minted}[fontsize=\scriptsize, breaklines=true]{markdown}
|   | Calls      | Count     |
|---|:-----------|----------:|
|   | Resolved   | 7,799,929 |
|   | Unresolved |   260,249 |
    \end{minted}
    \caption{Part of the expected results}
    \Description{Part of the expected results}
    \label{f:judge_execution_example_1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.30\textwidth}
    \begin{minted}[fontsize=\scriptsize, breaklines=true]{markdown}
|   | Calls      | Count     |
|---|:-----------|----------:|
|   | Resolved   | 7,799,929 |
|   | Unresolved |   168,482 |
    \end{minted}
    \caption{Part of the actual results}
    \Description{Part of the actual results}
    \label{f:judge_execution_example_2}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.30\textwidth}
    \begin{minted}[fontsize=\scriptsize, breaklines=true]{markdown}
|   | Calls      | Count     |
|---|:-----------|----------:|
|   | Resolved   | 7,799,929 |
|   | Unresolved |   ???,??? |
    \end{minted}
    \caption{Feedback Given to the Agents}
    \Description{Feedback Given to the Agents}
    \label{f:judge_execution_example_3}
  \end{subfigure}
  \caption{Example of the Result Mismatch Feedback for Table 2 of Drosos et al.~\cite{bloat}.}
  \label{f:judge_execution_example}
\end{figure}

Importantly, the judging mechanism reveals which parts of the result the agent got right, but not the parts it got wrong.
Figure~\ref{f:judge_execution_example}~gives an example of this feedback mechanism.
Suppose Figure~\ref{f:judge_execution_example_1}~is the expected result of the reproduction, and Figure~\ref{f:judge_execution_example_2}~is the actual result obtained after running the submitted ~\goodscript.
As results match for Resolved Count but not for Unresolved Count, Figure~\ref{f:judge_execution_example_3}~informs the agent that the result matches for the Resolved Count but not for the Unresolved Count.
This guides the agents on the successful parts but does not give away the expected results for the unsuccessful parts.
In addition, we allow the agents to submit and get feedback from the judging mechanism while they are still running.
This is made possible due to our feedback mechanism, which does not leak the expected results to the agent.

\subsubsection{LLM-Based Judging of Reproduction Method}
\label{s:judge_llm}

Even if the submitted~\goodscript~produces results that match the expected results, it is still possible that no real reproduction is performed and results are obtained through inappropriate means.
Thus, our second phase of judging adopts an LLM-as-Judge to grade how the reproduction is conducted.
The judging mechanism classifies three categories of the reproduction method:
\begin{itemize}
  \item \textbf{\copyrepro{}}: The submitted script just copies the results without actually reproducing.
  \item \textbf{\downrepro{}}: The submitted script performs lightweight reproduction, skipping some time-consuming steps.
  \item \textbf{\fullrepro{}}: The submitted script performs the full reproduction possible.
\end{itemize}

Figure~\ref{f:judge_llm_example}~gives an example of each variant.
In Figure~\ref{f:judge_llm_example_1}, the script directly copies the checked-in result for submission.
Our judging mechanism judges that this script is~\copyrepro, as there is no actual reproduction performed.
A warning is also given to the agent that no credit will be given with this submission.
In Figure~\ref{f:judge_llm_example_2}, the script uses checked-in raw data to process it to generate results.
Although this still falls short of full reproduction, we judge this to be a valid reproduction.
This is because the full reproduction of the given experiment takes a few hundred hours in this case, and checking if the analysis result of the checked-in raw data matches the paper result is still valuable.
In Figure~\ref{f:judge_llm_example_3}, the script does the full reproduction from raw data generation to perform analysis on one example.
This is the ideal case, especially if it can be achieved within a reasonable time.

\begin{figure}[t]
  \centering
  \begin{subfigure}[t]{1\textwidth}
    \begin{minted}[fontsize=\scriptsize, breaklines=true]{bash}
cp Unimocg_Artifact/summaries/summary_results_fingerprint.txt /workspace/repro.log
    \end{minted}
    \caption{Example~\copyrepro{}~scripts for Table 1 of Unimocg~\cite{DBLP:conf/se/HelmRKRM25}.}
    \Description{Example~\copyrepro{}~scripts for Table 1 of Unimocg~\cite{DBLP:conf/se/HelmRKRM25}.}
    \label{f:judge_llm_example_1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{1\textwidth}
    \begin{minted}[fontsize=\scriptsize, breaklines=true]{bash}
docker-compose run pmsat python parse_all_results_timeouts.py benchmarkingset-rc2-results > /workspace/repro.log
    \end{minted}
    \caption{Example~\downrepro{}~scripts for Table 3 of Wallner et al.~\cite{pmsat}.}
    \Description{Example~\downrepro{}~scripts for Table 3 of Wallner et al.~\cite{pmsat}.}
    \label{f:judge_llm_example_2}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{1\textwidth}
    \begin{minted}[fontsize=\scriptsize, breaklines=true]{bash}
docker-compose run pmsat /bin/bash -c "cd /pmsat-inference && python -m pip install 'numpy<2' && python run_pmsat_on_traces.py examples-results/ping_pong_example/info.json -nmax 7 && MPLBACKEND=Agg python parse_single_run_results.py TRACE-results/92e710ef352c4739cd7569794272588a" > /workspace/repro.log
    \end{minted}
    \caption{Example~\fullrepro{}~scripts for Table 2 of Wallner et al.~\cite{pmsat}.}
    \Description{Example~\fullrepro{}~scripts for Table 2 of Wallner et al.~\cite{pmsat}.}
    \label{f:judge_llm_example_3}
  \end{subfigure}
  \caption{Example of the reproduction mechanisms.}
  \label{f:judge_llm_example}
\end{figure}

\subsubsection{Enforcing Validation on Exit Attempts}

While developing~\approach, we observed that the LLM agent tries to prematurely exit its agentic loop although a judging mechanism we introduced (Section~\ref{s:judge}) gave the agent feedback on its errors.
To prevent this, when the LLM agent tries to exit, we run extra round of judging and if the submitted script does not pass the judging, we yield back the control to the agent to enforce it to try harder until it succeeds or reaches its limits.
This is inspired by similar mechanism employed by ExecutionAgent~\cite{DBLP:journals/pacmse/BouzeniaP25}.

\subsection{Implementation}
\label{s:implementation}

\paragraph{Separating Judge tool and Submit tool}

Initially, the agents were given the judge tool to self-grade the submissions themselves.
As the judge tool requires expected results to determine if the submission matches the expected results, this led to agents directly accessing the expected results themselves, bypassing the judging mechanism.
Thus, in our implementation, we use a client-server architecture where the agents use the Submit tool to submit the script to the server, whereas the Judge tool is the server that receives the incoming submitted script, grades the submission following the method described in Section~\ref{s:judge}, and returns the results to the Submit tool.
This prevents the leakage issue.
