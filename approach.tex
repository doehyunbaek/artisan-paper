% \section{Background}
% \label{s:background}

% \subsection{Artifact Evaluation}

% \subsection{LLM Agents for Software Engineering}

\section{Approach}
\label{s:approach}

We first give a brief overview of the approach (Section~\ref{s:overview}).
Then, we provide a detailed explanation of the three tools that we provide to the agent (Sections \ref{s:get}, \ref{s:format}, \ref{s:judge}).
Finally, we explain the implementation details (Section~\ref{s:implementation}).

\subsection{Overview}
\label{s:overview}

\begin{figure}[t]
  \centering
  \begin{minted}{markdown}
## Goal
Please reproduce the results reported in Table 6 from the SE paper.
<obfuscated_result>
**Table 6: Sensitivity of deactivating scheduled workflows to the parameter k.**

| k                     |    1 |    2 |    5 |   10 |   15 |   20 |
| --------------------- | ---: | ---: | ---: | ---: | ---: | ---: |
| Impact on VM time (%) | -?.? | -?.? | -?.? | -?.? | -?.? | -?.? |
</obfuscated_result>
The same result is available at `/workspace/obfuscated.md`.
The paper that the result is from is available at `/workspace/paper.md`.

## Workflow overview
1. **Download the artifact** from https://zenodo.org/records/10529665.
2. **Read the README** to understand reproduction steps.
3. **Locate relevant commands** for Table 6.
4. **Execute the required commands** step-by-step.
5. **Author the reproduction script** `repro.sh` using the template below.
  \end{minted}
  \caption{Example user prompt provided to the agent}
  \Description{Example user prompt provided to the agent}
  \label{f:user_prompt}
\end{figure}

\begin{figure}[t]
  \centering
  \begin{minted}{bash}
#!/bin/bash
artisan get https://zenodo.org/records/10529665 # Section 2.2: Get tool
<Commands to reproduce the results> # Filled out by LLM Agent later
artisan format obfuscated.md execution.txt # Section 2.3: Format tool
artisan judge repro.txt # Section 2.4: Judge tool
  \end{minted}
  \caption{Example script template provided to the agent.}
  \Description{Example script template provided to the agent.}
  \label{f:script_template}
\end{figure}

An LLM agent is an LLM-based system that uses tools to autonomously interact with the environment to achieve a given goal, provided in the form of a user prompt~\cite{DBLP:conf/icse/BouzeniaDP25}.
Figure~\ref{f:user_prompt} gives an example of a user prompt given to our LLM agent to task it to generate~\goodscript~that reproduces Table 6 of Bouzenia and Pradel~\cite{DBLP:conf/icse/BouzeniaP24}.

As a goal, the LLM agent is given a numerical result to reproduce, albeit with results obfuscated with question marks.
While developing our approach, we found that giving the results unobfuscated led to the undesirable behavior of directly copying the given results.
By giving results in an obfuscated form, we instruct LLM agents on what to reproduce but not on what the expected values are.
In addition, the content of the paper is provided to the agent, with the results to reproduce obfuscated as well.

As a workflow, the LLM agent is provided with a series of steps that should be taken to reproduce a numerical result from the research paper.
We only provide high-level objectives that should be taken at each step, and the LLM agent autonomously executes actions to achieve these objectives.
The five-step workflow provided to the agent is modeled after how researchers would tackle the problem of reproducing research artifacts themselves.
The last step of the workflow, authoring the reproduction script, is the key distinction of our approach compared to existing work.
As we try to automate the artifact evaluation by tasking LLM agents to generate~\goodscript{}, we provide a script template that agents should follow to guide the generation.

Figure~\ref{f:script_template} shows the script template we provide.
First, agents should fetch the artifact from different artifact repositories.
We provide the Get tool (Section~\ref{s:get}) to offer a unified interface to the artifact repositories.
Then, the agent autonomously exercises the fetched artifact to reproduce the research results.
As the commands needed to reproduce the results vary from artifact to artifact, we ask the agent to review the commands it has executed to fill in the necessary commands.
Next, the agent has to analyze and format the execution logs into the expected format.
To support this, we provide the Format tool (Section~\ref{s:format}), which is an LLM-based formatter that takes an expected results and execution log as input and outputs a formatted results.
Lastly, to guide the agent in reproducing the research result, we allow the agent to get feedback on the \goodscript~they have generated with the Judge tool (Section~\ref{s:judge}).
By calling the Judge tool, the agent can get feedback on whether (1) the output of the generated code matches the actual results, and (2) the way~\goodscript~generates the output is appropriate.
The agent can incorporate the feedback if either of the two conditions is not met, guiding the reproduction process.

\subsection{\emph{Get} tool: Fetching Research Artifacts}
\label{s:get}

There currently exist multiple different repository providers that researchers use to archive their artifacts, including Zenodo, Figshare, and GitHub.
Without a specialized tool, an LLM agent can still retrieve the artifact by utilizing CLI tools like \texttt{curl}.
However, we found that agents stumble when interacting with repository APIs initially.
In addition, complex probing of repository APIs makes the code generation task more complicated, leading to more errors.

To mitigate this, we implement the Get tool, which the agent can invoke with just the URL to fetch the research artifact.
It handles the repository-specific logic silently, downloads the research artifact, extracts the research artifact, and locates the README file.
It also supports caching of artifacts that have already been fetched.
While simple, we found that this is an effective strategy that helps the agent in initial fetching and also in generating~\goodscript.

\subsection{\emph{Format} tool: Formatting Execution Outputs into Results}
\label{s:format}

% \begin{figure}[t]
%   \centering
%   \begin{minted}{markdown}
% You format results into GitHub-flavored Markdown tables.

% CRITICAL TEMPLATE RULE:
% Return a table that is IDENTICAL to the provided expected table except that
% every placeholder question mark (?) is replaced by exactly one decimal digit
% (0-9). One ? => one digit. A pattern like "??.??" means two digits, a dot,
% two digits. A pattern like "?,???,???" means digits in those exact slots with
% commas preserved. Do NOT add, remove, or move pipes |, spaces, commas, dots,
% percent signs %, header separators, or rows. Do NOT alter any cell that
% contains no ? characters. Preserve capitalization and ordering exactly.

% DATA FILLING:
% Derive each replaced digit sequence from ONLY the reproduction text.
%   \end{minted}
%   \caption{Part of the system prompt used to implement the Format tool}
%   \Description{Part of the system prompt used to implement the Format tool}
%   \label{f:format}
% \end{figure}

\begin{figure}[t]
  \centering
  % First Subfigure
  \begin{subfigure}[t]{0.48\textwidth}
    \begin{minted}[fontsize=\scriptsize, breaklines=true]{markdown}
**Table 3: Evaluation Results**

| Project Name | Summary   | Annotations | Warnings |
| ---------------------    |             | -------- |
| MarginSwap   | (omitted) |           ? |        ? |
| PoolTogehter | (omitted) |           ? |        ? |
| Tracer       | (omitted) |           ? |        ? |
| Yield Micro  | (omitted) |           ? |        ? |

    \end{minted}
    \caption{Part of the obfuscated results provided as input.}
    \Description{Part of the obfuscated results provided as input.}
    \label{f:format_input_result}
  \end{subfigure}
  \hfill % Adds space between the figures
  % Second Subfigure
  \begin{subfigure}[t]{0.48\textwidth}
    \begin{minted}[fontsize=\scriptsize, breaklines=true]{bash}
Switched global version to 0.8.3
'solc --version' running
Reference: ScType
Error with TMP_57 in function updateHourlyBondAmount
Error with: TMP_57 in function updateHourlyBondAmount
Annotation count: 6
Function count: 20
Executing Group 1
[*] Tested 1 warning for MarginSwap
    \end{minted}
    \caption{Part of the execution output provided as input.}
    \Description{Part of the execution output provided as input.}
    \label{f:format_input_log}
  \end{subfigure}
  \caption{Example input given to the Format tool.}
  \label{f:format_input}
\end{figure}

Even after an agent has successfully reproduced research results by running a series of commands, the outputs of the commands are usually not exactly the same as what is presented in the paper.
Some artifacts also provide scripts that process these execution outputs into the format used in the paper, but many artifacts do not.
Of course, an LLM agent can try to write the missing processing script itself, but we found that it struggles to write such scripts in many cases.
Often, an LLM agent successfully ran commands necessary to reproduce the research results but could not get the correct score from the judge because the formatting of the execution outputs was inadequate.

To address this, we implement the Format tool, which is an LLM-based formatter that takes an obfuscated results and execution log as input and outputs a formatted results.
Figure~\ref{f:format_input} shows an example input that is passed to the Format tool.
While the research results are structured in a tabular format (Figure~\ref{f:format_input_result}), the output of the execution is just a log file (Figure~\ref{f:format_input_log}), lacking structure.
In addition, the log file is 1,103 lines in total.
The Format tool passes the obfuscated results and execution output as inputs with few-shot examples to the LLM, and outputs the results with the expected values filled in.

\subsection{\emph{Judge} tool: Evaluating Reproduction Attempts}
\label{s:judge}

In previous work developing output-based agents~\cite{super, corebench}, we found two characteristics that were inadequate for our problem formulation:
(1) The agents do not get feedback on their submission while they are working on the problem.
This becomes problematic in our case, as generating code that reproduces the result is a more complex problem and has more room for error than outputting just the result.
(2) How the agents obtain the output is not judged.
We observe many~\fastpath~that agents take while attempting to reproduce the result (Section~\ref{s:fastpath}), and proper consideration of this phenomenon is important for the accurate assessment of the LLM agents' capabilities.
Thus, we propose a two-tier judging system which addresses both problems: execution-based judging of the code output (Section~\ref{s:judge_execution})~and LLM-based judging of the reproduction method (Section~\ref{s:judge_llm}).

\subsubsection{Execution-Based Judging of Reproduction Scripts}
\label{s:judge_execution}

In our approach, LLM agents generate and submit a shell script that runs independently of any agent-related machinery.
Given the submission, our execution-based judging mechanism runs the submitted script in a fresh container environment.
The judging mechanism classifies three cases where the submitted script fails judging in this phase:
(1) Static Error: when the submitted script fails to run because of a syntax error, the judging mechanism judges the script as a static error and gives the error message as feedback.
We also consider cases where the script is missing from the expected path as static error.
(2) Runtime Error: when the submitted script encounters an error while running, the judging mechanism judges the script as a runtime error and returns which command caused the error with the error message.
(3) Result Mismatch: when the submitted script runs to completion without any error but the result differs from the expected values, the judging mechanism judges the script as a result mismatch.

\begin{figure}[t]
  \centering
  \begin{subfigure}[t]{0.30\textwidth}
    \begin{minted}[fontsize=\scriptsize, breaklines=true]{markdown}
|   | Calls      | Count     |
|---|:-----------|----------:|
|   | Resolved   | 7,799,929 |
|   | Unresolved |   260,249 |
    \end{minted}
    \caption{Expected Results}
    \Description{Expected Results}
    \label{f:judge_execution_example_1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.30\textwidth}
    \begin{minted}[fontsize=\scriptsize, breaklines=true]{markdown}
|   | Calls      | Count     |
|---|:-----------|----------:|
|   | Resolved   | 7,799,929 |
|   | Unresolved |   168,482 |
    \end{minted}
    \caption{Actual Results}
    \Description{Actual Results}
    \label{f:judge_execution_example_2}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.30\textwidth}
    \begin{minted}[fontsize=\scriptsize, breaklines=true]{markdown}
|   | Calls      | Count     |
|---|:-----------|----------:|
|   | Resolved   | 7,799,929 |
|   | Unresolved |   ???,??? |
    \end{minted}
    \caption{Feedback Given to the Agents}
    \Description{Feedback Given to the Agents}
    \label{f:judge_execution_example_3}
  \end{subfigure}
  \caption{Example of the Result Mismatch Feedback}
  \label{f:judge_execution_example}
\end{figure}

Importantly, the judging mechanism reveals which parts of the result the agent got right, but not the parts it got wrong.
Figure~\ref{f:judge_execution_example}~gives an example of this feedback mechanism.
Suppose Figure~\ref{f:judge_execution_example_1}~is the expected result of the reproduction, and Figure~\ref{f:judge_execution_example_2}~is the actual result obtained after running the submitted ~\goodscript.
As results match for Resolved Count but not for Unresolved Count, Figure~\ref{f:judge_execution_example_3}~informs the agent that the result matches for the Resolved Count but not for the Unresolved Count.
This guides the agents on the successful parts but does not give away the expected results for the unsuccessful parts.
In addition, we allow the agents to submit and get feedback from the judging mechanism while they are still running.
This is made possible due to our feedback mechanism, which does not leak the expected results to the agent.

\subsubsection{LLM-Based Judging of Reproduction Method}
\label{s:judge_llm}

Even if the submitted~\goodscript~produces results that match the expected results, it is still possible that no real reproduction is performed and results are obtained through inappropriate means.
Thus, our second phase of judging adopts an LLM-as-Judge to grade how the reproduction is conducted.
The judging mechanism classifies three categories of the reproduction method:
(1) \copyrepro{}: The submitted script just copies the results without actually reproducing.
(2) \downrepro{}: The submitted script performs lightweight reproduction, skipping some time-consuming steps.
(3) \fullrepro{}: The submitted script performs the full reproduction possible.

Figure~\ref{f:judge_llm_example}~gives an example of each variant.
In Figure~\ref{f:judge_llm_example_1}, the script directly copies the checked-in result for submission.
Our judging mechanism judges that this script is~\copyrepro, as there is no actual reproduction performed.
A warning is also given to the agent that no credit will be given with this submission.
In Figure~\ref{f:judge_llm_example_2}, the script uses checked-in raw data to process it to generate results.
Although this still falls short of full reproduction, we judge this to be a valid reproduction.
This is because the full reproduction of the given experiment takes a few hundred hours in this case, and checking if the analysis result of the checked-in raw data matches the paper result is still valuable.
In Figure~\ref{f:judge_llm_example_3}, the script does the full reproduction from raw data generation to perform analysis on one example.
This is the ideal case, especially if it can be achieved within a reasonable time.

\begin{figure}[t]
  \centering
  \begin{subfigure}[t]{1\textwidth}
    \begin{minted}[fontsize=\scriptsize, breaklines=true]{bash}
cat artifact/InvCon+/results.txt > /workspace/repro.txt
    \end{minted}
    \caption{Example~\copyrepro{}~scripts.}
    \Description{Example~\copyrepro{}~scripts.}
    \label{f:judge_llm_example_1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{1\textwidth}
    \begin{minted}[fontsize=\scriptsize, breaklines=true]{bash}
docker-compose run pmsat python parse_all_results_timeouts.py benchmarkingset-rc2-results > /workspace/repro.txt
    \end{minted}
    \caption{Example~\downrepro{}~scripts.}
    \Description{Example~\downrepro{}~scripts.}
    \label{f:judge_llm_example_2}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{1\textwidth}
    \begin{minted}[fontsize=\scriptsize, breaklines=true]{bash}
docker-compose run pmsat /bin/bash -c "cd /pmsat-inference && python -m pip install 'numpy<2' && python run_pmsat_on_traces.py examples-results/ping_pong_example/info.json -nmax 7 && MPLBACKEND=Agg python parse_single_run_results.py TRACE-results/92e710ef352c4739cd7569794272588a" > /workspace/repro.txt
    \end{minted}
    \caption{Example~\fullrepro{}~scripts.}
    \Description{Example~\fullrepro{}~scripts.}
    \label{f:judge_llm_example_3}
  \end{subfigure}
  \caption{Example of the reproduction mechanisms.}
  \label{f:judge_llm_example}
\end{figure}

\subsection{Implementation}
\label{s:implementation}

\paragraph{Agent Framework and LLM Model Choice}

We use mini-swe-agent~\cite{minisweagent}, version 1.17.1 for the base agent framework when building~\approach.
We implement the tools in Python, version 3.12.10.
Although mini-swe-agent supports any models supported by LiteLLM~\cite{litellm}, we mainly experimented with gpt-5-mini-2025-08-07 for our day-to-day experimentation and include gpt-5.1-2025-11-13 and gpt-5-nano-2025-08-07 for full-scale evaluation.

\paragraph{Enforcing Validation on Exit Attempts}

While developing~\approach, we observed that the LLM agent tries to prematurely exit its agentic loop although a judging mechanism we introduced (Section~\ref{s:judge}) gave the agent feedback on its errors.
To prevent this, when the LLM agent tries to exit, we run extra round of judging and if the submitted script does not pass the judging, we yield back the control to the agent to enforce it to try harder until it succeeds or reaches its limits.
This is inspired by similar mechanism employed by ExecutionAgent~\cite{DBLP:journals/pacmse/BouzeniaP25}.

\paragraph{Separating Judge tool and Submit tool}

Initially, the agents were given the judge tool to self-grade the submissions themselves.
This led to undesirable results as agents could access the expected results themselves, leaking the results.
Thus, in our implementation, we use a client-server architecture where the agents use the Submit tool to submit the script to the server, whereas the Judge tool is the server that receives the incoming submitted script, grades the submission following the method described in Section~\ref{s:judge}, and returns the results to the Submit tool.
This prevents the leakage issue.
