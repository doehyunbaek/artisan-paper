% \section{Background}
% \label{s:background}

% \subsection{Artifact Evaluation}

% \subsection{LLM Agents for Software Engineering}

\section{Approach}
\label{s:approach}

We first give a brief overview of the approach (Section ~\ref{s:overview}).
Then, we give a detailed explanation of the three tools that we provide to the agent (Section \ref{s:get}, \ref{s:format}, \ref{s:judge})

\subsection{Overview}
\label{s:overview}

\begin{figure}[t]
  \centering
  \begin{minted}{bash}
#!/usr/bin/bash
# Section 1: Artifact download
artisan get {{ artifact_url }}
# Section 2: Reproduction commands (populate from reviewed steps)
<Commands to reproduce the table>
# Section 3: Formatting and submission block
artisan format --expected /workspace/expected.md --repro /workspace/repro.txt
  \end{minted}
  \caption{Script template provided to the agent to guide the code generation.}
  \Description{Script template provided to the agent to guide the code generation.}
  \label{f:script_template}
\end{figure}

\begin{figure}[t]
  \centering
  \begin{minted}{bash}
cat > /workspace/repro_{{ artifact_name }}_table_{{ table_index }}.sh <<'EOF'
{{ script_template }}
EOF
artisan judge /workspace/repro_{{ artifact_name }}_table_{{ table_index }}.sh
  \end{minted}
  \caption{Comand template provided to the agent to guide the script submission.}
  \Description{Comand template provided to the agent to guide the script submission.}
  \label{f:command_template}
\end{figure}

As \approach~tries to automate the artifact evaluation by tasking LLM agents to generate~\goodscript{}, we provide a script template that agents should follow for effective research reproduction.
Figure \ref{f:script_template} shows a script template we provide.
First, agents should fetch the artifact from different artifact repositories.
We provide Get tool (Section~\ref{s:get}) to provide a unified interface to the artifact repositories.
Then, the agent autonomously exercises the fetched artifact to reproduce the research results.
As the commands needed to reproduce the results vary from artifact to artifact, we ask the agent to review the commands it has executed to fill in the necessary commands.
Lastly, the agent has to analyze and format the execution logs into the expected format.
To support this, we provide Format tool (Section~\ref{s:format}), which is an LLM-based formatter that takes an expected table and execution log as an input and output a formatted table.

To guide the agent in reproducing the research result, we allow agent to get a feedback on the \goodscript~they have generated with Judge tool (Section~\ref{s:judge}).
Figure \ref{f:command_template} shows a command template we provide to the agent.
By writing the generated script to a file and calling a Judge tool, the agent can get feedback whether (1) the output of the generated code matches the actual table, and (2) the way~\goodscript~generates the output is appropriate.
The agent can incorporate the feedback if any of the two conditions are not met, guiding the reproduction process.

\subsection{\emph{Get} tool: Fetching Research Artifacts}
\label{s:get}

There currently exists multiple different repository providers that researchers use to archive their artifacts, including Zenodo, Figshare, and GitHub.
Without the specizlied tool, LLM agent can still retrive the artifact from the given tool by utilizing CLI tools like curl to retrieve the artifact.
However, we found that the agent stumbles on interacting with the repository API initially.
In addition, complex probing of the repository API makes the code generation task more complicated, leading to more errors.

To mitigate this, we implement Get tool, which agent can invoke with just the url to fetch the research artifact.
It handles the repository-specific logic silently, downloads the research artifact, extracts the research artifact, and locate the README file of the artifact.
It also supports caching of the artifact that has been already fetched before.
While simple, we found that this is an effective strategy that helps the agent in initial fetching and also generating~\goodscript.

\subsection{\emph{Format} tool: Formatting Execution Outputs into Results}
\label{s:format}

% \begin{figure}[t]
%   \centering
%   \begin{minted}{markdown}
% You format results into GitHub-flavored Markdown tables.

% CRITICAL TEMPLATE RULE:
% Return a table that is IDENTICAL to the provided expected table except that
% every placeholder question mark (?) is replaced by exactly one decimal digit
% (0-9). One ? => one digit. A pattern like "??.??" means two digits, a dot,
% two digits. A pattern like "?,???,???" means digits in those exact slots with
% commas preserved. Do NOT add, remove, or move pipes |, spaces, commas, dots,
% percent signs %, header separators, or rows. Do NOT alter any cell that
% contains no ? characters. Preserve capitalization and ordering exactly.

% DATA FILLING:
% Derive each replaced digit sequence from ONLY the reproduction text.
%   \end{minted}
%   \caption{Part of the system prompt used to implement the Format tool}
%   \Description{Part of the system prompt used to implement the Format tool}
%   \label{f:format}
% \end{figure}

\begin{figure}[t]
  \centering
  % First Subfigure
  \begin{subfigure}[t]{0.48\textwidth}
    \begin{minted}[fontsize=\scriptsize, breaklines=true]{markdown}
**Table 3: Evaluation Results**

| Project Name | Summary   | Annotations | Warnings |
| ---------------------    |             | -------- |
| MarginSwap   | (omitted) |           ? |        ? |
| PoolTogehter | (omitted) |           ? |        ? |
| Tracer       | (omitted) |           ? |        ? |
| Yield Micro  | (omitted) |           ? |        ? |

    \end{minted}
    \caption{Part of the obfuscated table provided as input.}
    \Description{Part of the obfuscated table provided as input.}
    \label{f:format_input_table}
  \end{subfigure}
  \hfill % Adds space between the figures
  % Second Subfigure
  \begin{subfigure}[t]{0.48\textwidth}
    \begin{minted}[fontsize=\scriptsize, breaklines=true]{bash}
Switched global version to 0.8.3
'solc --version' running
Reference: ScType
Error with TMP_57 in function updateHourlyBondAmount
Error with: TMP_57 in function updateHourlyBondAmount
Annotation count: 6
Function count: 20
Executing Group 1
[*] Tested 1 warning for MarginSwap
    \end{minted}
    \caption{Part of the execution output provided as input.}
    \Description{Part of the execution output provided as input.}
    \label{f:format_input_log}
  \end{subfigure}
  \caption{Example input given to the Format tool.}
  \label{f:format_input}
\end{figure}

Even after an agent has successfully reproduced research results by running a series of command, the outputs of the commands are usually not exactly the same as in what is presented in the paper.
Some artifacts also provide scripts that processes these execution outputs into one that is used in the paper, but many artifacts do not provide scripts.
Of course, an LLM agent can try to write the missing processing script itself, but we found that it struggles to write such scripts in a lot of cases.
In many cases, an LLM successfully ran commands necessary to reproduce the research results, but could not get correct score from the judge as the formatting of the execution outputs was inadequate.

To address this, we implement Format tool, which is an LLM-based formatter that takes an expected table and execution log as an input and output a formatted table.
% Figure \ref{f:format} shows a part of the system prompt that we use to implement the Format tool and 
Figure \ref{f:format_input} shows an example input that is passed to a Format tool.
You can see that while table expects the research result in a tabular format (Figure \ref{f:format_input_table}), the output of the execution is just a log file (Figure \ref{f:format_input_log}), and there is not much structure.
In addition, the log file is 1,103 lines in total.
Format tool passes expected table and execution output as inputs with some few-shot examples to the LLM, and outputs the tables with expected values filled-in.

\subsection{\emph{Judge} tool: Evaluating Reproduction Attempts}
\label{s:judge}

In previous work which develops output-based agents~\cite{super, corebench}, we found two characteristics that we found inadequate in our problem formulation:
(1) The agents do not get feedback on their submission while they are working on the problem.
This becomes problematic in our case, as generating code that reproduces the result is more complex problem and has more room for error than outputting just the result.
(2) How the agents obtain the output is not judged.
We observe many~\fastpath~that agents take while attempting to reproduce the result (Section \ref{s:fastpath}), and judge proper consideration of this phenoenon is important for accurate assessment of the LLM agents' capabilities.
Thus, we propose two-tier judging system which addresses both problem, by execution-based judging of the submission~\ref{s:judge_execution}~and LLM-based judging of the reproduction method~\ref{s:judge_llm}.
\subsubsection{Execution-Based Judging of Reproduction Scripts}
\label{s:judge_execution}

In our approach, LLM agents generate and submit a shell script that runs independently of any agent-related machinaries.
Given the submission, our execution-based judging mechanism runs the submitted script in fresh container environment.
Judging mechanism classify three cases where the submitted script fails judging in this phase.
(1) Static Error: when the submitted script fails to run because of syntax error, judging mechanism judge the script as a static error and give the error message as a feedback.
(2) Runtime Error: when the submitted script encounters error while running, judging mechanism judge the script as a runtime error and return which command caused error with what error message.
(3) Result Mismatch: when the submitted script runs to completion without any error but the result is different with the expected values, judging mechanism judge the script as a result mismatch.

\begin{figure}[t]
  \centering
  \begin{subfigure}[t]{0.30\textwidth}
    \begin{minted}[fontsize=\scriptsize, breaklines=true]{markdown}
|   | Calls      | Count     |
|---|:-----------|----------:|
|   | Resolved   | 7,799,929 |
|   | Unresolved |   260,249 |
    \end{minted}
    \caption{Expected Results}
    \Description{Expected Results}
    \label{f:judge_execution_example_1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.30\textwidth}
    \begin{minted}[fontsize=\scriptsize, breaklines=true]{markdown}
|   | Calls      | Count     |
|---|:-----------|----------:|
|   | Resolved   | 7,799,929 |
|   | Unresolved |   168,482 |
    \end{minted}
    \caption{Results of the Submitted~\goodscript}
    \Description{Results of the Submitted~\goodscript}
    \label{f:judge_execution_example_2}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{0.30\textwidth}
    \begin{minted}[fontsize=\scriptsize, breaklines=true]{markdown}
|   | Calls      | Count     |
|---|:-----------|----------:|
|   | Resolved   | 7,799,929 |
|   | Unresolved |   ???,??? |
Result mismatch: some cells differ
    \end{minted}
    \caption{Feedback Given to the Agents}
    \Description{Feedback Given to the Agents}
    \label{f:judge_execution_example_3}
  \end{subfigure}
  \caption{Example of the Result Mismatch Feedback}
  \label{f:judge_execution_example}
\end{figure}

Importantly, judging mechanism reveals which parts of the result the agent got right, but not the parts it got wrong.
Figure~\ref{f:judge_execution_example}~gives example of this feedback mechanism.
Suppose Figure~\ref{f:judge_execution_example_1}~is the expected result of the reproduction, and Figure~\ref{f:judge_execution_example_2}~is the result obtained after running the submitted ~\goodscript.
As results match for Resolved Count but not for Unresolved Count, Figure~\ref{f:judge_execution_example_3}~informs the agent that the result matches for the Resolved Count but not for Unresolved Count.
This guides the agents for the successful parts but not give away the expected the results for the unsuccessful parts.

In addition, we allow the agents to submit and get the feedback from the judging mechanism while they are still running, different from previous work~\cite{super, corebench}.
This is because:
(1) There is more room for error in the code generation task; we do not want to punish the agents for generating slightly wrong~\goodscript~although it successfully executed commands to reproduce the results
(2) Our judging mechanism does not allow the agents to gain hints on the expected results to guess the results.

\subsubsection{LLM-Based Judging of Reproduction Method}
\label{s:judge_llm}

Even if the submitted~\goodscript~produces the results that match the expected results, it is still possible that no real reproduction is performed and results are obtained through inappropriate means.
Thus, our second phase of judging adopts an LLM-as-Judge to grade how the reproduction is conducted.
Judging mechanism classify three categories of the reproduction method:
(1) \copyrepro{}: The submitted script just copies the results without actually reproducing.
(2) \downrepro{}: The submitted script performs lightweight reproduction using the checked in raw data.
(3) \fullrepro{}: The submitted script perform the full reproduction possible.

Figure~\ref{f:judge_llm_example}~gives example of each variants.
In Figure~\ref{f:judge_llm_example_1}, the script directly copies the checked in result for submission.
Our judging mechanism judges that this script is~\copyrepro, as there is no actual reproduction performed.
Warning is also given to the agent that no credit will be given with this submission.
In Figure~\ref{f:judge_llm_example_2}, the script uses checked-in raw data to process it to generate results.
Although this still fall short of the full reproduction, we judge this to be a still valid reproduction.
This is because the full reproduction of the given experiment takes few hundred hours and checking if the analysis result of the checked-in raw data match the paper result is still valuable.
In Figure~\ref{f:judge_llm_example_3}, the script does the full reproduction from raw data generation to analysis on one example.
This is the ideal case, especially if it can be achieved within a reasonable time.

\begin{figure}[t]
  \centering
  \begin{subfigure}[t]{1\textwidth}
    \begin{minted}[fontsize=\scriptsize, breaklines=true]{bash}
# Section 3: Formatting and submission block
cat artifact/InvCon+/results.txt > /workspace/repro.txt
    \end{minted}
    \caption{Example~\copyrepro{}~scripts.}
    \Description{Example~\copyrepro{}~scripts.}
    \label{f:judge_llm_example_1}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{1\textwidth}
    \begin{minted}[fontsize=\scriptsize, breaklines=true]{bash}
# Section 3: Formatting and submission block
docker-compose run pmsat python parse_all_results_timeouts.py benchmarkingset-rc2-results > /workspace/repro.txt
    \end{minted}
    \caption{Example~\downrepro{}~scripts.}
    \Description{Example~\downrepro{}~scripts.}
    \label{f:judge_llm_example_2}
  \end{subfigure}
  \hfill
  \begin{subfigure}[t]{1\textwidth}
    \begin{minted}[fontsize=\scriptsize, breaklines=true]{bash}
# Section 3: Formatting and submission block
docker-compose run pmsat /bin/bash -c "cd /pmsat-inference && python -m pip install 'numpy<2' && python run_pmsat_on_traces.py examples-results/ping_pong_example/info.json -nmax 7 && MPLBACKEND=Agg python parse_single_run_results.py TRACE-results/92e710ef352c4739cd7569794272588a" > /workspace/repro.txt
    \end{minted}
    \caption{Example~\fullrepro{}~scripts.}
    \Description{Example~\fullrepro{}~scripts.}
    \label{f:judge_llm_example_3}
  \end{subfigure}
  \caption{Example of the reproduction mechanisms.}
  \label{f:judge_llm_example}
\end{figure}

\section{Implementation}
\label{s:implementation}

\paragraph{Agent Framework and LLM Model Choice}

We use mini-swe-agent~\cite{minisweagent}, version 1.17.1 to build \approach.
Although mini-swe-agent supports any models supported by LiteLLM~\cite{litellm}, we mainly experimented with gpt-5-mini-2025-08-07 for our day-to-day experimentation and include gpt-5.1-2025-11-13 and gpt-5-nano-2025-08-07 for full-scale evaluation.

\paragraph{Separating Judge tool and Submit tool}

Initially, the agents were given the judge tool to self-grade the submissions themselves.
This led to undesirable results as agents could access the expected results themselves, leaking the results.
Thus, in our implementation, we use a client-server architecture that the agents use Submit tool to submit the script to the server, whereas the Judge tool is the server that receives the incoming submitted script, grades the submission following the method described in Section~\ref{s:judge}, and returns the results to the Submit tool.
This prevents the leakage issue.
