\section{Benchmark}
\label{s:benchmark}

In this section, we present the methodology used to construct \benchmark.
We first state the requirements we target (Section~\ref{s:requirements}), then describe paper selection (Section~\ref{s:paper_select}), task selection (Section~\ref{s:task_select}), an example task (Section~\ref{s:task_example}), and our grading scheme (Section~\ref{s:grading}).

\subsection{Requirements}
\label{s:requirements}

We impose two core requirements on \benchmark.
(1) Tasks should closely reflect real-world artifact evaluation so that performance on the benchmark generalizes to practical use.
This addresses limitations of prior benchmarks~\cite{DBLP:conf/emnlp/BoginYG0BCSK24,DBLP:journals/tmlr/SiegelKNSN24,DBLP:conf/acl/HuZLWPK25}, which we find do not fully mirror artifact evaluation as practiced in the software engineering community.
(2) Manual validation is required.
Although SWE-Bench~\cite{jimenez2024swebench} is influential for evaluating LLMs on real software engineering issues, subsequent efforts such as SWE-Bench Verified~\cite{chowdhury2024swebenchverified} and~\cite{DBLP:journals/corr/abs-2503-15223} highlight the importance of human verification to avoid overestimating or underestimating agent capabilities.
These requirements introduce trade-offs.
For example, the task suite must be broad enough to cover diverse use cases yet small enough to permit manual validation.
Further, very large tasks increase time and monetary cost, which can be prohibitive for resource-constrained academic groups~\cite{DBLP:conf/iclr/ChanCJASMSLMPMW25}.

\subsection{Paper Selection}
\label{s:paper_select}

\begin{table}[t]
  \caption{Paper selection criteria.}
  \label{t:paper_select}
  \centering
  \begin{tabular}{@{}l r@{}}
    \toprule
    Criterion & Count \\
    \midrule
    Flagship SE conference papers (2024) with \emph{Available} and \emph{Reusable} badges & 114 \\
    Packaged using Docker & 73 \\
    No non-public API use & 65 \\
    No GPU use & 42 \\
    Less than eight hours per task & 31 \\
    Manual reproduction successful & 24 \\
    \bottomrule
  \end{tabular}
\end{table}

Guided by these requirements, we curate a set of \papersetsize~papers following the process in Table~\ref{t:paper_select}.
(1) We gather 114 papers from four flagship software engineering venues (ICSE, FSE, ASE, ISSTA) in 2024 with ACM \emph{Artifacts Available} and \emph{Artifacts Evaluated—Reusable} badges~\cite{acm-artifact-badging-v1_1}.
We then manually locate links to the accompanying artifacts.
For FSE and ISSTA, the ACM Digital Library lists artifact links; for other venues, we use a web search (e.g., Google) with the paper title.
(2) We keep 76 papers whose artifacts are packaged using Docker.
This reflects the fact that Docker and related virtualization technologies are now standard in artifact evaluation~\cite{icse2024-ae,fse2024-ae,ase2024-ae}.
To implement this, we case-insensitively search Markdown and PDF files for the term ``docker''.\footnote{\url{https://github.com/doehyunbaek/artisan/blob/bc7e6cd/evaluation/select_papers.py\#L234}}
(3) We exclude papers that rely on non-public APIs to ensure the benchmark is reproducible.
To implement this, we search Python files and Jupyter notebooks for ``openai'' or ``anthropic''.\footnote{\url{https://github.com/doehyunbaek/artisan/blob/bc7e6cd/evaluation/select_papers.py\#L276}}
All eight excluded papers use OpenAI APIs.
(4) We exclude artifacts requiring GPUs or specialized hardware to maintain accessibility across hardware setups.
We search for common GPU keywords (e.g., \texttt{cuda}, \texttt{cudnn}, \texttt{tensorflow}, \texttt{nvidia-smi}, \texttt{gpus}) while ignoring non-code extensions (e.g., \texttt{.txt}, \texttt{.csv}, \texttt{.json}, \texttt{.log}).\footnote{\url{https://github.com/doehyunbaek/artisan/blob/bc7e6cd/evaluation/select_papers.py\#L312}}
(5) We require that each task can be completed within eight hours.
We search the paper text for the keyword ``hour'' and exclude papers that explicitly mention timeouts beyond eight hours.
(6) Finally, we manually attempt to reproduce each paper’s major claim within an eight-hour budget; failures to set up the environment or reproduce the claim lead to exclusion.

\subsection{Task Selection}
\label{s:task_select}

From \papersetsize~papers selected, we curate \tasksetsize~tasks that is given to LLM agents as a single task instance.
We first give general criteria (Section~\ref{s:task_selection_criteria}), and give two notable distinctions about inconsistensies between paper and artifacts (Section~\ref{s:inconsistencies}) and slow path and fast path (Section~\ref{s:slow_and_fast}).

\subsubsection{Task Selection criteria}
\label{s:task_selection_criteria}

\subsubsection{Inconsistensies Between Paper and Artifacts}
\label{s:inconsistencies}

\subsubsection{nondeterministic Behaviors of Artifacts}
\label{s:nondeterminism}

ofc timing numbers are different run-to-run and from run-to-paper.
We repeat ten times and only keep the parts which stays the same.

\subsubsection{Slow Path and Fast Path}
\label{s:slow_and_fast}

\subsection{Example Task}
\label{s:task_example}

% (Your example task description goes here.)

\subsection{Grading}
\label{s:grading}

% (Your grading rubric and procedure go here.)
