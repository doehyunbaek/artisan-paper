\section{Benchmark}
\label{s:benchmark}

In this section, we present the methodology used to construct \benchmark.
We first state the requirements we target (Section~\ref{s:requirements}), then describe paper selection (Section~\ref{s:paper_select}), task selection (Section~\ref{s:task_select}), an example task (Section~\ref{s:task_example}), and our grading scheme (Section~\ref{s:grading}).

\subsection{Requirements}
\label{s:requirements}

We impose two core requirements on \benchmark.
(1) Tasks should closely reflect real-world artifact evaluation so that performance on the benchmark generalizes to practical use.
This addresses limitations of prior benchmarks~\cite{DBLP:conf/emnlp/BoginYG0BCSK24,DBLP:journals/tmlr/SiegelKNSN24,DBLP:conf/acl/HuZLWPK25}, which we find do not fully mirror artifact evaluation as practiced in the software engineering community.
(2) Manual validation is required.
Although SWE-Bench~\cite{jimenez2024swebench} is influential for evaluating LLMs on real software engineering issues, subsequent efforts such as SWE-Bench Verified~\cite{chowdhury2024swebenchverified} and~\cite{DBLP:journals/corr/abs-2503-15223} highlight the importance of human verification to avoid overestimating or underestimating agent capabilities.
These requirements introduce trade-offs.
For example, the task suite must be broad enough to cover diverse use cases yet small enough to permit manual validation.
Further, very large tasks increase time and monetary cost, which can be prohibitive for resource-constrained academic groups~\cite{DBLP:conf/iclr/ChanCJASMSLMPMW25}.

\subsection{Paper Selection}
\label{s:paper_select}

\begin{table}[t]
  \caption{Paper selection criteria.}
  \label{t:paper_select}
  \centering
  \begin{tabular}{@{}l r@{}}
    \toprule
    Criterion & Count \\
    \midrule
    Flagship SE conference papers (2024) with \emph{Available} and \emph{Reusable} badges & 114 \\
    Packaged using Docker & 72 \\
    No non-public API use & 65 \\
    No GPU use & 46 \\
    Less than eight hours per task & 35 \\
    Manual reproduction successful & \papersetsize \\
    \bottomrule
  \end{tabular}
\end{table}

Guided by these requirements, we curate a set of \papersetsize~papers following the process in Table~\ref{t:paper_select}.
(1) We gather 114 papers from four flagship software engineering venues (ICSE, FSE, ASE, ISSTA) in 2024 with ACM \emph{Artifacts Available} and \emph{Artifacts Evaluated—Reusable} badges~\cite{acm-artifact-badging-v1_1}.
We then manually locate links to the accompanying artifacts.
For FSE and ISSTA, the ACM Digital Library lists artifact links; for other venues, we use a web search (e.g., Google) with the paper title.
(2) We keep 76 papers whose artifacts are packaged using Docker.
This reflects the fact that Docker and related virtualization technologies are now standard in artifact evaluation~\cite{icse2024-ae,fse2024-ae,ase2024-ae}.
To implement this, we case-insensitively search Markdown and PDF files for the term ``docker''.\footnote{\url{https://github.com/doehyunbaek/artisan/blob/bc7e6cd/evaluation/select_papers.py\#L234}}
(3) We exclude papers that rely on non-public APIs to ensure the benchmark is reproducible.
To implement this, we search Python files and Jupyter notebooks for ``openai'' or ``anthropic''.\footnote{\url{https://github.com/doehyunbaek/artisan/blob/bc7e6cd/evaluation/select_papers.py\#L276}}
All eight excluded papers use OpenAI APIs.
(4) We exclude artifacts requiring GPUs or specialized hardware to maintain accessibility across hardware setups.
We search for common GPU keywords (e.g., \texttt{cuda}, \texttt{cudnn}, \texttt{tensorflow}, \texttt{nvidia-smi}, \texttt{gpus}) while ignoring non-code extensions (e.g., \texttt{.txt}, \texttt{.csv}, \texttt{.json}, \texttt{.log}).\footnote{\url{https://github.com/doehyunbaek/artisan/blob/bc7e6cd/evaluation/select_papers.py\#L312}}
(5) We require that each task can be completed within eight hours.
We search the paper text for the keyword ``hour'' and exclude papers that explicitly mention timeouts beyond eight hours.
Out of eleven papers excluded, seven papers use 24 hours timeout, three papers use 12 hours timeout, and one paper uses 125 hour timeout.
(6) Finally, we manually attempt to reproduce each paper’s major claim within an eight-hour budget; failures to set up the environment or reproduce the claim lead to exclusion.
Out of seven papers excluded, two are excluded as most of the evaluation dataset is missing, two are excluded as raw data is missing, two are excluded as script to generate results from raw data are missing, and one is excluded as docker image mentioned in the artifact is no longer accessible.

\begin{table}[t]
  \centering
  \begin{minipage}[t]{0.48\textwidth}
    \centering
    \caption{Software engineering techniques covered by our paper selection.}
    \label{t:topics}
    \input{tables/topic_distribution}
  \end{minipage}\hfill
  \begin{minipage}[t]{0.48\textwidth}
    \centering
    \caption{Programming languages in the selected papers.}
    \label{t:languages}
    \input{tables/language_distribution}
  \end{minipage}
\end{table}

Table~\ref{t:topics} shows the distribution of software engineering techniques covered by our paper selection.
Overall, there is a diverse selection of software engineering techniques.
One notable exception is a possible under-representation of AI4SE and SE4AI techniques.
This is mainly due to our exclusion of papers using non-public APIs and GPUs.
Still, there is one paper~\cite{DBLP:conf/kbse/LeeJL24} which makes use of classical machine learning technique (Random forest and XGBoost) which does not make use of either non-public APIs and GPUs.
Table~\ref{t:languages} shows the distribution of programming languages covered by our paper seleciton.
Overall, Python is the most prevalent, comprising 57\% of the papers.
However, there is a sizable representation of other languages, notably Java, Rust, Ocaml, Scala, C, and even Bash, which were not covered in the previous benchmarks~~\cite{DBLP:conf/emnlp/BoginYG0BCSK24, DBLP:journals/tmlr/SiegelKNSN24, DBLP:conf/acl/HuZLWPK25}.

\subsection{Task Selection}
\label{s:task_select}

\begin{table}[t]
  \caption{Task selection criteria.}
  \label{t:task_select}
  \centering
  \begin{tabular}{@{}l r@{}}
    \toprule
    Criterion & Count \\
    \midrule
    Total number of tables & 114 \\
    No non-result tables & 101 \\
    Manual reproduction successful & \tablesetsize \\
    \bottomrule
  \end{tabular}
\end{table}

From \papersetsize~papers selected, we curate \tablesetsize~tasks that is given to LLM agents as a single task instance.
We first give general criteria (Section~\ref{s:task_selection_criteria}), and give two notable distinctions about inconsistensies between paper and artifacts (Section~\ref{s:inconsistencies}) and slow path and fast path (Section~\ref{s:slow_and_fast}).

\subsubsection{Task Selection criteria}
\label{s:task_selection_criteria}

\subsubsection{Inconsistensies Between Paper and Artifacts}
\label{s:inconsistencies}

\subsubsection{Slow Path and Fast Path}
\label{s:slow_and_fast}

\subsection{Example Task}
\label{s:task_example}

% (Your example task description goes here.)

\subsection{Grading}
\label{s:grading}

Unlike previous work~~\cite{DBLP:conf/emnlp/BoginYG0BCSK24,DBLP:journals/tmlr/SiegelKNSN24} we allow submission and getting the feedback whle agent is running.
This helps improve the effectiveness of the agent, as shown in ~\ref{s:eval_judging}

% (Your grading rubric and procedure go here.)
