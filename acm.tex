\documentclass[acmsmall,screen,review,anonymous]{acmart}

\usepackage[capitalise,nameinlink]{cleveref}
\usepackage[many]{tcolorbox}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{pifont}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{hyperref}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10011007.10011006.10011073</concept_id>
       <concept_desc>Software and its engineering~Software maintenance tools</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}
\ccsdesc[500]{Software and its engineering~Software maintenance tools}
\keywords{Artifact Evaluation, Benchmarks, LLM agents}

\begin{document}

\newcommand\benchmark{SEAE-Bench}
\newcommand\approach{Artisan}

\newcommand\goodscript{particular reproduction script}
\newcommand\newbug{paper-artifact inconsistencies}
\newcommand\fastpath{fast-path}
\newcommand\slowpath{slow-path}

\newcommand\papersetsize{30}
\newcommand\tasksetsize{162}

\newcommand\todo[1]{\textcolor{red}{TODO: #1}}
\newcommand\evalgoodscript{\textcolor{red}{TODO}}
\newcommand\evalgoodscriptoutperform{\textcolor{red}{TODO}}
\newcommand\nondetsize{\textcolor{red}{TODO}}
\newcommand\inconsistenciessize{\textcolor{red}{TODO}}
\newcommand\fastpathsize{\textcolor{red}{TODO}}

\title{Artisan: Automated Artifact Evaluation with Agents}
\begin{abstract}
Artifact evaluation has been shown to be a valuablue way to ensure the reproducibility of software artifacts.
However, in its current form, it remains notoriously labor-intensive.
To mitigate this, recent work has introduced benchmarks that evaluate LLM agents on automatically reproducing research results.
We identify several limitations of existing benchmarks:
(1) They largely frame the task as output prediction.
(2) Their usefulness is limited: LLM agents successfully predicting the outcome of research does not raise the confidence that the paper is reproducible.
(3) They cover a narrow domain of software engineering techniques and programming languages.
To address these issues, we present \benchmark, an automated artifact evaluation benchmark based on code generation.
\benchmark~has the following characteristics:
(1) It measures an agent’s ability to generate \emph{\goodscript{}}, a script that reproduces a paper’s particular claim; not only outputs but also the script used to produce them are graded.
(2) It uncovers \emph{\newbug{}}, where the result of \goodscript{} does not match with the results of the paper, the presence of \goodscript{} acts as a credible evidence of the transcription or rounding errors on the side of the paper.
(3) It enables distinctions of \emph{\fastpath{}}, a reproduction using pre-computed values, and \emph{\slowpath{}}, a reproduction from scratch.
Informed by existing LLM agents’ struggles on \benchmark, we develop Artisan, an agentic approach to automated artifact evaluation.
Artisan includes specialized tools for domain-specific challenges of artifact evaluation.
Our experiments show that, while most LLM agents struggle on \benchmark, Artisan is effective, producing \evalgoodscript{} \goodscript{}~and outperforming the baseline by \evalgoodscriptoutperform{}.
In addition, during the development of \benchmark, we uncovered \inconsistenciessize~\newbug{}, \fastpathsize~\fastpath{}~that artifacts provide, and \nondetsize~non-deterministic behaviors of the artifact which is not explicitly documented, demonstrating the strength of our approach to benchmark construction.
\end{abstract}
\maketitle

\input{intro}
\input{background}
\input{benchmark}
\input{approach}
\input{evaluation}
\input{outro}

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
\endinput
