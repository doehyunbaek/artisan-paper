\documentclass[acmsmall,screen,review,anonymous]{acmart}

\usepackage[capitalise,nameinlink]{cleveref}
\usepackage[many]{tcolorbox}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{pifont}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{hyperref}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10011007.10011006.10011073</concept_id>
       <concept_desc>Software and its engineering~Software maintenance tools</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}
\ccsdesc[500]{Software and its engineering~Software maintenance tools}
\keywords{Artifact Evaluation, Benchmarks, LLM agents}

\begin{document}

\newcommand\benchmark{SEAE-Bench}
\newcommand\approach{Artisan}

\newcommand\particularscrip{particular reproduction script}
\newcommand\newbug{paper-artifact inconsistencies}
\newcommand\fastpath{fast-path}
\newcommand\slowpath{slow-path}

\newcommand\papersetsize{28}
\newcommand\tablesetsize{101}
\newcommand\particularscriptsize{\textcolor{red}{TODO}}
\newcommand\particularscriptsizeoutperform{\textcolor{red}{TODO}}
\newcommand\inconsistenciessize{2}
\newcommand\fastpathsize{\textcolor{red}{TODO}}

\newcommand\todo[1]{\textcolor{red}{TODO: #1}}

\title{Artisan: Automated Artifact Evaluation with Agents}
\begin{abstract}
Artifact evaluation has become a standard practice in software engineering and programming language community to ensure the reproducibility of research artifacts.
However, in its current form, it remains notoriously labor-intensive.
Recent work has introduced benchmarks that evaluate LLM agents on automatically reproducing research artifaccts.
Not only are they limited in terms of software engineering techniques and programming languages they cover, they are not well-suited to augment the current practice of artifact evaluation.
(1) They largely frame the task as output prediction.
(2) LLM agents successfully predicting the outcome of research does not raise the confidence that the artifact is reproducible.
To address these issues, we present \benchmark, an automated artifact evaluation benchmark based on code generation.
\benchmark~has the following characteristics:
(1) It measures an agent’s ability to generate \emph{\particularscrip{}}, a script that reproduces a paper’s particular claim.
(2) It uncovers \emph{\newbug{}}, where the result of \particularscrip{} does not match with the results of the paper, the presence of \particularscrip{} acts as a credible evidence of the transcription or rounding errors on the side of the paper.
(3) It enables distinctions of \emph{\fastpath{}}, a reproduction using pre-computed values, and \emph{\slowpath{}}, a reproduction from scratch.
In addition, \benchmark~covers diverse software engineering techniques and real-world programming languages.
Informed by existing LLM agents’ struggles on \benchmark, we develop Artisan, an agentic approach to automated artifact evaluation.
Artisan includes specialized tools for domain-specific challenges of artifact evaluation.
Our experiments show that, while most LLM agents struggle on \benchmark, Artisan is effective, producing \particularscriptsize{} \particularscrip{}~and outperforming the baseline by \particularscriptsizeoutperform{}.
In addition, during the development of \benchmark, we uncovered \inconsistenciessize~\newbug{}, and \fastpathsize~\fastpath{}~that artifacts provide demonstrating the strength of our approach to benchmark construction.
\end{abstract}
\maketitle

\input{intro}
\input{background}
\input{benchmark}
\input{approach}
\input{evaluation}
\input{outro}

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
\endinput
