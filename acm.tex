\documentclass[acmsmall,screen,review,anonymous]{acmart}

\usepackage[capitalise,nameinlink]{cleveref}
\usepackage[many]{tcolorbox}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{pifont}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{hyperref}
\usepackage{minted}

\settopmatter{printacmref=false}
\renewcommand\footnotetextcopyrightpermission[1]{}
\setcopyright{none}
% \begin{CCSXML}
% <ccs2012>
%    <concept>
%        <concept_id>10011007.10011006.10011073</concept_id>
%        <concept_desc>Software and its engineering~Software maintenance tools</concept_desc>
%        <concept_significance>500</concept_significance>
%        </concept>
%  </ccs2012>
% \end{CCSXML}
% \ccsdesc[500]{Software and its engineering~Software maintenance tools}
% \keywords{Artifact Evaluation, LLM agents, Benchmarks}

\begin{document}

\author{Doehyun Baek}
\email{doehyunbaek@gmail.com}
\orcid{0009-0004-0117-1060}
\affiliation{
  \institution{University of Stuttgart}
  \city{Stuttgart}
  \country{Germany}
}
\author{Michael Pradel}
\orcid{0000-0003-1623-498X}
\email{michael@binaervarianz.de}
\affiliation{
  \institution{CISPA Helmholtz Center for Information Security}
  \city{Stuttgart}
  \country{Germany}
}

\newcommand\approach{Artisan}
\newcommand\benchmark{Artisan-Bench}

\newcommand\goodscript{reproduction script}
\newcommand\newbug{paper-artifact inconsistencies}
\newcommand\fastpath{fast-path}

\newcommand\copyrepro{Copy Results}
\newcommand\downrepro{Last-mile Reproduction}
\newcommand\fullrepro{Full Reproduction}

\newcommand\papersetsize{23}
\newcommand\tablesetsize{60}
\newcommand\goodscripttsize{41}
\newcommand\goodscripttsizeoutperform{2.93}
\newcommand\artisancost{\$0.407}
\newcommand\artisantime{eight minutes}
\newcommand\newbugsize{21}
\newcommand\fastpathsize{30}

\newcommand\todo[1]{\textcolor{red}{TODO: #1}}

\title{Artisan: Automated Artifact Evaluation with Agents}
\begin{abstract}
Artifact evaluation has become a standard practice in the software engineering community to ensure the reproducibility of research results.
However, the current manual process remains notoriously labor-intensive.
To mitigate the manual effort, we present \approach, the first automated artifact evaluation approach utilizing an LLM agent tasked with code generation.
% that frames the task as a code generation task.
Given numerical results from a paper, \approach~autonomously takes actions to reproduce the results.
Subsequently, \approach~encapsulates this knowledge into a \emph{\goodscript}, which can be run independently to reproduce the results.
If the results mismatch, it could be that the error stems from either the paper or the artifact, which we term \emph{\newbug}.
If the results match, our judging mechanism uses an LLM-as-judge to evaluate the completeness of the reproduction.
This mechanism precludes trivial~\goodscript s that merely copies checked-in results, which we term \emph{\fastpath}.
% The core difference of~\artisan~and the current state-of-the-art is its realistic problem formulation and task formulation as code generation task.
% To mitigate the manual effort, recent work has explored using LLM agents to automatically reproduce research results.
% However, these approaches mostly frame the problem as an output prediction task, which (1) cannot distinguish between agent errors and artifact errors when results mismatch and (2) ignores how the output is obtained.
To evaluate \approach, we introduce \benchmark, the first benchmark assessing LLM agents' capabilities to generate \goodscript s.
\benchmark~comprises 60 tasks derived from 23 software engineering papers.
All tasks in \benchmark~are manually validated for reproducibility and cover diverse software engineering techniques and programming languages.
Our experiments show that \approach~is effective, producing~\goodscripttsize{}~new reproduction scripts and outperforming the baseline, mini-swe-agent, by \goodscripttsizeoutperform{}$\times$ in terms of  s generated while taking~\artisancost~and~\artisantime~on average per task.
Furthermore, applying \approach~to~\benchmark~uncovered~\newbugsize~\newbug~and~\fastpathsize~\fastpath, demonstrating the practical utility of our approach.
\end{abstract}
\maketitle

\input{intro}
\input{approach}
\input{benchmark}
\input{evaluation}
\input{outro}

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
\endinput
