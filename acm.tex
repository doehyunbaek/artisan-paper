\documentclass[acmsmall,screen,review,anonymous]{acmart}

\usepackage[capitalise,nameinlink]{cleveref}
\usepackage[many]{tcolorbox}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{pifont}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{hyperref}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10011007.10011006.10011073</concept_id>
       <concept_desc>Software and its engineering~Software maintenance tools</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}
\ccsdesc[500]{Software and its engineering~Software maintenance tools}
\keywords{Artifact Evaluation, LLM agents, Benchmarks}

\begin{document}

\newcommand\approach{Artisan}
\newcommand\benchmark{Artisan-Bench}

\newcommand\goodscript{reproduction script}
\newcommand\newbug{paper-artifact inconsistencies}
\newcommand\fastpath{fast-path}
\newcommand\slowpath{slow-path}

\newcommand\papersetsize{28}
\newcommand\tablesetsize{101}
\newcommand\goodscripttsize{46}
\newcommand\newbugsize{20}
\newcommand\fastpathsize{38}
\newcommand\goodscripttsizeoutperform{4.6}

\newcommand\todo[1]{\textcolor{red}{TODO: #1}}

\title{Artisan: Automated Artifact Evaluation with Agents}
\begin{abstract}
Artifact evaluation has become a standard practice in the research community to ensure the reproducibility of research artifacts.
However, in its current form, it remains notoriously labor-intensive.
To mitigate this, recent work has attempted using LLM agents to automatically reproduce research results.
But, they mostly frame the task as output prediction task, which (1) does not consider how the output is obtained (2) can not distinguish error on the side of agent or artifact when the results do not match.
To address this, we present \approach, an automated artifact evaluation approach that frames the task as code generation task.
Given a numerical result of the research paper, \approach~autonomously takes actions to reproduce the result.
After the course of action, \approach~encapsulates this knowledge into a \emph{\goodscript}, which can be run independently to reproduce research results.
If the results match, our judging mechanism also evaluates \goodscript{}~on how the result is obtained, by using LLM-as-Judge that evaluates the completeness of the reproduction.
This mechanism precludes trivial~\goodscript~that just copies the checked-in research results, which we call \emph{\fastpath}.
If the results mismatch, it could be that the error is on the side of either paper or artifact, which we call \emph{\newbug}
The presence of \goodscript, which runs independently of LLM, acts as a credible evidence if this the case.
To evaluate \approach, we present \benchmark, a new benchmark that evaluates LLM agents' capabilities to generate \goodscript.
\benchmark~comprises of 61 tasks that are derived from 23 Software Engineering papers.
All the tasks in \benchmark~are manually validated to be reproducible and it covers diverse software engineering techniques and programming languages.
Our experiments show that Artisan is effective, producing \goodscripttsize{} \goodscript{}~and outperforming the baseline by \goodscripttsizeoutperform{}$\times$.
Furthermore, during the course of applying \approach~to \benchmark, we uncovered \fastpathsize~\fastpath{}~ and \newbugsize~\newbug{}, demonstrating the strength of our approach.
\end{abstract}
\maketitle

\input{intro}
\input{background}
\input{approach}
\input{benchmark}
\input{evaluation}
\input{outro}

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
\endinput
