\documentclass[acmsmall,screen,review,anonymous]{acmart}

\usepackage[capitalise,nameinlink]{cleveref}
\usepackage[many]{tcolorbox}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{pifont}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{hyperref}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10011007.10011006.10011073</concept_id>
       <concept_desc>Software and its engineering~Software maintenance tools</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}
\ccsdesc[500]{Software and its engineering~Software maintenance tools}
\keywords{Artifact Evaluation, Benchmarks, LLM agents}

\begin{document}

\newcommand\benchmark{SEAE-Bench}


\newcommand\todo[1]{\textcolor{red}{TODO: #1}}
\newcommand\artisanpartial{\textcolor{red}{TODO}}
\newcommand\artisanpartialoutperform{\textcolor{red}{TODO}}
\newcommand\inconsistencies{\textcolor{red}{TODO}}

\title{Artisan: Automated Artifact Evaluation with Agents}
\begin{abstract}
Artifact evaluation has been shown to be a valuablue way to ensure the reproducibility of software artifacts.
However, in its current form, it remains notoriously labor-intensive.
To mitigate this, recent work has introduced benchmarks that evaluate LLM agents on automatically reproducing research results.
We identify several limitations of existing benchmarks:
(1) They largely frame the task as output prediction.
(2) Their usefulness is limited: LLM agents successfully predicting the outcome of research does not raise the confidence that the paper is reproducible.
(3) They cover a narrow domain of software engineering techniques and programming languages.
To address these issues, we present \benchmark, an automated artifact evaluation benchmark for software engineering research.
\benchmark~has the following characteristics:
(1) It measures an agent’s ability to generate code that reproduces a paper’s particular claim; not only outputs but also the script used to produce them are graded.
(2) The submitted script serves as a reproduction script for the particular claim, and when the result of the script does not match with the results of the paper, it acts as a credible evidence of the transcription or rounding errors on the side of the paper.
(3) It spans diverse software engineering techniques and programming languages.
Informed by existing LLM agents’ struggles on \benchmark, we develop Artisan, an agentic approach to automated artifact evaluation.
Artisan includes specialized tools for domain-specific challenges of artifact evaluation.
Our experiments show that, while most LLM agents struggle on \benchmark, Artisan is effective, producing \artisanpartial{} reproduction scripts and outperforming the baseline by \artisanpartialoutperform{}.
In addition, during the development of \benchmark, we uncovered \inconsistencies{} inconsistencies between papers and their artifacts, demonstrating the strength of our approach to benchmark construction.
\end{abstract}
\maketitle

\input{intro}
\input{background}
\input{benchmark}
\input{approach}
\input{evaluation}
\input{outro}

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
\endinput
