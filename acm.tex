\documentclass[acmsmall,screen,review,anonymous]{acmart}

\usepackage[capitalise,nameinlink]{cleveref}
\usepackage[many]{tcolorbox}
\usepackage{listings}
\usepackage{enumitem}
\usepackage{pifont}
\usepackage{subcaption}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{multirow}
\usepackage{makecell}
\usepackage{hyperref}

\begin{CCSXML}
<ccs2012>
   <concept>
       <concept_id>10011007.10011006.10011073</concept_id>
       <concept_desc>Software and its engineering~Software maintenance tools</concept_desc>
       <concept_significance>500</concept_significance>
       </concept>
 </ccs2012>
\end{CCSXML}
\ccsdesc[500]{Software and its engineering~Software maintenance tools}
\keywords{Artifact Evaluation, Benchmarks, LLM agents}

\begin{document}

\newcommand\benchmark{SEA-Bench}
\newcommand\approach{Artisan}

\newcommand\particularscrip{reproduction script}
\newcommand\newbug{paper-artifact inconsistencies}
\newcommand\fastpath{fast-path}
\newcommand\slowpath{slow-path}

\newcommand\papersetsize{28}
\newcommand\tablesetsize{101}
\newcommand\particularscriptsize{46}
\newcommand\inconsistenciessize{26}
\newcommand\fastpathsize{38}
\newcommand\particularscriptsizeoutperform{4.6}

\newcommand\todo[1]{\textcolor{red}{TODO: #1}}

\title{Artisan: Automated Artifact Evaluation with Agents}
\begin{abstract}
Artifact evaluation has become a standard practice in the software engineering and programming languages communities to ensure the reproducibility of research artifacts.
However, in its current form, it remains notoriously labor-intensive.
Recent work has introduced benchmarks that evaluate LLM agents on automatically reproducing research artifacts.
Not only are they limited in terms of the software engineering techniques and programming languages they cover, but they also exhibit the following conceptual limitations:
(1) They largely frame the task as output prediction.
(2) LLM agents successfully predicting the outcome of research does not raise the confidence that the artifact is reproducible.
(3) They do not distinguish how the results are reproduced.
To address these issues, we present \benchmark, an automated artifact evaluation benchmark based on code generation.
\benchmark~has the following characteristics:
(1) It measures an agent’s ability to generate \emph{\particularscrip{}}, a script that reproduces a paper’s particular claim.
(2) It uncovers \emph{\newbug{}}, where the result of \particularscrip{} does not match the results of the paper.
In the case of a mismatch, the presence of \particularscrip{} acts as credible evidence of error in either the paper or the artifact.
(3) It distinguishes between \emph{\fastpath{}}, a reproduction using checked-in data, and \emph{\slowpath{}}, a reproduction from scratch.
In addition, \benchmark~covers diverse software engineering techniques and real-world programming languages.
Informed by existing LLM agents’ struggles on \benchmark, we develop Artisan, an agentic approach to automated artifact evaluation.
Artisan includes specialized tools for the domain-specific challenges of artifact evaluation.
Our experiments show that, while baseline LLM agents struggle on \benchmark, Artisan is effective, producing \particularscriptsize{} new \particularscrip{}~and outperforming the baseline by \particularscriptsizeoutperform{}$\times$.
Furthermore, during the development of \benchmark, we uncovered \inconsistenciessize~\newbug{} and \fastpathsize~\fastpath{} that artifacts provide, demonstrating the strength of our approach to benchmark construction.
\end{abstract}
\maketitle

\input{intro}
\input{background}
\input{benchmark}
\input{approach}
\input{evaluation}
\input{outro}

\bibliographystyle{ACM-Reference-Format}
\bibliography{references}

\end{document}
\endinput
